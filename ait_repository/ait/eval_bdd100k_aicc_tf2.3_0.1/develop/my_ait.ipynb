{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# AIT Development notebook\n",
    "\n",
    "\n",
    "## notebook of structure\n",
    "\n",
    "|#|area name|cell num|description|edit or not|\n",
    "|---|---|---|---|---|\n",
    "| 1|flags set|1|setting of launch jupyter or ait flag.|no edit|\n",
    "| 2|ait-sdk install|1|Use only jupyter launch.<br>find ait-sdk and install.|no edit|\n",
    "| 3|create requirements and pip install|3|Use only jupyter launch.<br>create requirements.txt.<br>And install by requirements.txt.|should edit(second cell, you set use modules.)|\n",
    "| 4|import|2|you should write use import modules.<br>but bottom lines do not edit.|should edit(first cell, you import your moduel.)|\n",
    "| 5|create manifest|1|Use only jupyter launch.<br>create ait.manifest.json.|should edit|\n",
    "| 6|create input|1|Use only jupyter launch.<br>create ait.input.json.|should edit|\n",
    "| 7|initialize|1|this cell is initialize for ait progress.|no edit|\n",
    "| 8|functions|N|you defined measures, resources, downloads in ait.manifesit.json. <br>Define any functions to add these.|should edit|\n",
    "| 9|main|1|Read the data set or model and calls the function defined in `functions-area`.|should edit|\n",
    "|10|entrypoint|1|Call the main function.|no edit|\n",
    "|11|license attribute set|1|Use only notebook launch.<br>Setting attribute for license.|should edit|\n",
    "|12|prepare deploy|1|Use only notebook launch.<br>Convert to python programs and create dag.py.|no edit|\n",
    "\n",
    "## notebook template revision history\n",
    "\n",
    "### 1.0.1 2020/10/21\n",
    "\n",
    "* add revision history\n",
    "* separate `create requirements and pip install` editable and noeditable\n",
    "* separate `import` editable and noeditable\n",
    "\n",
    "### 1.0.0 2020/10/12\n",
    "\n",
    "* new cerarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:flags set\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "# Determine whether to start AIT or jupyter by startup argument\n",
    "import sys\n",
    "is_ait_launch = (len(sys.argv) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (20.3.3)\n",
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Processing ./ait_sdk-0.1.5-py3-none-any.whl\n",
      "Collecting keras<=2.4.3\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting nbconvert<=6.0.7\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "\u001b[K     |████████████████████████████████| 552 kB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting entrypoints>=0.2.2\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jinja2>=2.4\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl (32 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.1-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting jupyter-client>=6.1.5\n",
      "  Downloading jupyter_client-6.1.7-py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-core\n",
      "  Downloading jupyter_core-4.7.0-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nbformat<=5.0.8\n",
      "  Downloading nbformat-5.0.8-py3-none-any.whl (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema!=2.5.0,>=2.4\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=17.4.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 9.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting numpy<=1.19.3\n",
      "  Downloading numpy-1.19.3-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.4.3.tar.gz (16 kB)\n",
      "Collecting psutil<=5.7.3\n",
      "  Downloading psutil-5.7.3.tar.gz (465 kB)\n",
      "\u001b[K     |████████████████████████████████| 465 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py-cpuinfo<=7.0.0\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pygments>=2.4.1\n",
      "  Downloading Pygments-2.7.3-py3-none-any.whl (950 kB)\n",
      "\u001b[K     |████████████████████████████████| 950 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.17.3.tar.gz (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyzmq>=13\n",
      "  Downloading pyzmq-20.0.0-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.14\n",
      "  Downloading scipy-1.5.4-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 10.3 MB/s eta 0:00:01     |█████████████████████████████▍  | 23.6 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.11.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tornado>=4.1\n",
      "  Downloading tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB)\n",
      "\u001b[K     |████████████████████████████████| 427 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting traitlets>=4.2\n",
      "  Downloading traitlets-5.0.5-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 10.7 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting bleach\n",
      "  Downloading bleach-3.2.1-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[K     |████████████████████████████████| 145 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting defusedxml\n",
      "  Downloading defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipython-genutils\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting nest-asyncio\n",
      "  Downloading nest_asyncio-1.4.3-py3-none-any.whl (5.3 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting testpath\n",
      "  Downloading testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-51.1.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pandocfilters, psutil, py-cpuinfo, pyrsistent, pyyaml\n",
      "  Building wheel for pandocfilters (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=7991 sha256=a76734377a827052331eb6d4baf81fb8071dab54b2a078d80622da96c1554605\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9e19v6c/wheels/fc/39/52/8d6f3cec1cca4ceb44d658427c35711b19d89dbc4914af657f\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.3-cp38-cp38-linux_x86_64.whl size=295132 sha256=5cbc266f0a5db2e16d1889aaea547402535a5c298d905b2a5821699566ae9c5e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9e19v6c/wheels/f6/59/c2/38111ef4c354088a156bc95fbeb5396c0cac91a0f62f7158b9\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-py3-none-any.whl size=20068 sha256=0ecb45b0116a280b5ff791fbbfca9951de6ef67aef2ef0674f2aa819f6675608\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9e19v6c/wheels/b4/99/9f/8eb77fdf759c1380719071722f2c37dd0fa1f6aa477c51cb6c\n",
      "  Building wheel for pyrsistent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp38-cp38-linux_x86_64.whl size=132290 sha256=370048e592ea638474f63ffc986348929993311e64aa7764a0b72146ddedc74b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9e19v6c/wheels/3d/22/08/7042eb6309c650c7b53615d5df5cc61f1ea9680e7edd3a08d2\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl size=44617 sha256=bcd9274b81d453b2c9f4ee99019e6cb73bbcb50ac82a403e4b0cd20ece8d2c1b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9e19v6c/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c\n",
      "Successfully built pandocfilters psutil py-cpuinfo pyrsistent pyyaml\n",
      "Installing collected packages: ipython-genutils, traitlets, six, setuptools, pyrsistent, attrs, tornado, pyzmq, python-dateutil, pyparsing, jupyter-core, jsonschema, webencodings, pygments, packaging, numpy, nest-asyncio, nbformat, MarkupSafe, jupyter-client, async-generator, testpath, scipy, pyyaml, pandocfilters, nbclient, mistune, jupyterlab-pygments, jinja2, h5py, entrypoints, defusedxml, bleach, py-cpuinfo, psutil, nbconvert, keras, ait-sdk\n",
      "  Attempting uninstall: ipython-genutils\n",
      "    Found existing installation: ipython-genutils 0.2.0\n",
      "    Uninstalling ipython-genutils-0.2.0:\n",
      "      Successfully uninstalled ipython-genutils-0.2.0\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.0.5\n",
      "    Uninstalling traitlets-5.0.5:\n",
      "      Successfully uninstalled traitlets-5.0.5\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 51.0.0\n",
      "    Uninstalling setuptools-51.0.0:\n",
      "      Successfully uninstalled setuptools-51.0.0\n",
      "  Attempting uninstall: pyrsistent\n",
      "    Found existing installation: pyrsistent 0.17.3\n",
      "    Uninstalling pyrsistent-0.17.3:\n",
      "      Successfully uninstalled pyrsistent-0.17.3\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.3.0\n",
      "    Uninstalling attrs-20.3.0:\n",
      "      Successfully uninstalled attrs-20.3.0\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.1\n",
      "    Uninstalling tornado-6.1:\n",
      "      Successfully uninstalled tornado-6.1\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 20.0.0\n",
      "    Uninstalling pyzmq-20.0.0:\n",
      "      Successfully uninstalled pyzmq-20.0.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.7.0\n",
      "    Uninstalling jupyter-core-4.7.0:\n",
      "      Successfully uninstalled jupyter-core-4.7.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 3.2.0\n",
      "    Uninstalling jsonschema-3.2.0:\n",
      "      Successfully uninstalled jsonschema-3.2.0\n",
      "  Attempting uninstall: webencodings\n",
      "    Found existing installation: webencodings 0.5.1\n",
      "    Uninstalling webencodings-0.5.1:\n",
      "      Successfully uninstalled webencodings-0.5.1\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.7.2\n",
      "    Uninstalling Pygments-2.7.2:\n",
      "      Successfully uninstalled Pygments-2.7.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.4.3\n",
      "    Uninstalling nest-asyncio-1.4.3:\n",
      "      Successfully uninstalled nest-asyncio-1.4.3\n",
      "  Attempting uninstall: nbformat\n",
      "    Found existing installation: nbformat 5.0.8\n",
      "    Uninstalling nbformat-5.0.8:\n",
      "      Successfully uninstalled nbformat-5.0.8\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 6.1.7\n",
      "    Uninstalling jupyter-client-6.1.7:\n",
      "      Successfully uninstalled jupyter-client-6.1.7\n",
      "  Attempting uninstall: async-generator\n",
      "    Found existing installation: async-generator 1.10\n",
      "    Uninstalling async-generator-1.10:\n",
      "      Successfully uninstalled async-generator-1.10\n",
      "  Attempting uninstall: testpath\n",
      "    Found existing installation: testpath 0.4.4\n",
      "    Uninstalling testpath-0.4.4:\n",
      "      Successfully uninstalled testpath-0.4.4\n",
      "  Attempting uninstall: pandocfilters\n",
      "    Found existing installation: pandocfilters 1.4.2\n",
      "    Uninstalling pandocfilters-1.4.2:\n",
      "      Successfully uninstalled pandocfilters-1.4.2\n",
      "  Attempting uninstall: nbclient\n",
      "    Found existing installation: nbclient 0.5.1\n",
      "    Uninstalling nbclient-0.5.1:\n",
      "      Successfully uninstalled nbclient-0.5.1\n",
      "  Attempting uninstall: mistune\n",
      "    Found existing installation: mistune 0.8.4\n",
      "    Uninstalling mistune-0.8.4:\n",
      "      Successfully uninstalled mistune-0.8.4\n",
      "  Attempting uninstall: jupyterlab-pygments\n",
      "    Found existing installation: jupyterlab-pygments 0.1.2\n",
      "    Uninstalling jupyterlab-pygments-0.1.2:\n",
      "      Successfully uninstalled jupyterlab-pygments-0.1.2\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.2\n",
      "    Uninstalling Jinja2-2.11.2:\n",
      "      Successfully uninstalled Jinja2-2.11.2\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.3\n",
      "    Uninstalling entrypoints-0.3:\n",
      "      Successfully uninstalled entrypoints-0.3\n",
      "  Attempting uninstall: defusedxml\n",
      "    Found existing installation: defusedxml 0.6.0\n",
      "    Uninstalling defusedxml-0.6.0:\n",
      "      Successfully uninstalled defusedxml-0.6.0\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 3.2.1\n",
      "    Uninstalling bleach-3.2.1:\n",
      "      Successfully uninstalled bleach-3.2.1\n",
      "  Attempting uninstall: nbconvert\n",
      "    Found existing installation: nbconvert 6.0.7\n",
      "    Uninstalling nbconvert-6.0.7:\n",
      "      Successfully uninstalled nbconvert-6.0.7\n",
      "Successfully installed MarkupSafe-1.1.1 ait-sdk-0.1.5 async-generator-1.10 attrs-20.3.0 bleach-3.2.1 defusedxml-0.6.0 entrypoints-0.3 h5py-3.1.0 ipython-genutils-0.2.0 jinja2-2.11.2 jsonschema-3.2.0 jupyter-client-6.1.7 jupyter-core-4.7.0 jupyterlab-pygments-0.1.2 keras-2.4.3 mistune-0.8.4 nbclient-0.5.1 nbconvert-6.0.7 nbformat-5.0.8 nest-asyncio-1.4.3 numpy-1.19.3 packaging-20.8 pandocfilters-1.4.3 psutil-5.7.3 py-cpuinfo-7.0.0 pygments-2.7.3 pyparsing-2.4.7 pyrsistent-0.17.3 python-dateutil-2.8.1 pyyaml-5.3.1 pyzmq-20.0.0 scipy-1.5.4 setuptools-51.1.0 six-1.15.0 testpath-0.4.4 tornado-6.1 traitlets-5.0.5 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# area:ait-sdk install\n",
    "# do not edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    # get ait-sdk file name\n",
    "    from pathlib import Path\n",
    "    from glob import glob\n",
    "    import re\n",
    "\n",
    "    def numericalSort(value):\n",
    "        numbers = re.compile(r'(\\d+)')\n",
    "        parts = numbers.split(value)\n",
    "        parts[1::2] = map(int, parts[1::2])\n",
    "        return parts\n",
    "    latest_sdk_file_path=sorted(glob('../lib/*.whl'), key=numericalSort)[-1]\n",
    "\n",
    "    ait_sdk_name = Path(latest_sdk_file_path).name\n",
    "    \n",
    "    # copy to develop dir\n",
    "    import shutil\n",
    "    current_dir = %pwd\n",
    "    shutil.copyfile(f'../lib/{ait_sdk_name}', f'{current_dir}/{ait_sdk_name}')\n",
    "\n",
    "    # install ait-sdk\n",
    "    !pip install --upgrade pip\n",
    "    !pip install --force-reinstall ./$ait_sdk_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create requirements and pip install\n",
    "# do not edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_requirements_generator import AITRequirementsGenerator\n",
    "    requirements_generator = AITRequirementsGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create requirements and pip install\n",
    "# should edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    requirements_generator.add_package('Cython')\n",
    "    requirements_generator.add_package('docopt')\n",
    "    requirements_generator.add_package('clint')\n",
    "    requirements_generator.add_package('crontab')\n",
    "    requirements_generator.add_package('tablib')\n",
    "    requirements_generator.add_package('matplotlib')\n",
    "    requirements_generator.add_package('Pillow')\n",
    "    requirements_generator.add_package('pycocotools')\n",
    "    requirements_generator.add_package('tensorflow','2.3.0')\n",
    "    requirements_generator.add_package('lxml')\n",
    "    requirements_generator.add_package('tf_slim')\n",
    "    requirements_generator.add_package('pandas')\n",
    "    requirements_generator.add_package('numpy')\n",
    "    requirements_generator.add_package('ipython')\n",
    "    requirements_generator.add_package('zipp','3.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r /workdir/root/develop/requirements.txt (line 13)) (1.19.3)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.8/site-packages (from -r /workdir/root/develop/requirements.txt (line 14)) (7.19.0)\n",
      "Processing ./ait_sdk-0.1.5-py3-none-any.whl\n",
      "Requirement already satisfied: keras<=2.4.3 in /opt/conda/lib/python3.8/site-packages (from ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (2.4.3)\n",
      "Requirement already satisfied: nbconvert<=6.0.7 in /opt/conda/lib/python3.8/site-packages (from ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (6.0.7)\n",
      "Requirement already satisfied: nbformat<=5.0.8 in /opt/conda/lib/python3.8/site-packages (from ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (5.0.8)\n",
      "Requirement already satisfied: psutil<=5.7.3 in /opt/conda/lib/python3.8/site-packages (from ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (5.7.3)\n",
      "Requirement already satisfied: py-cpuinfo<=7.0.0 in /opt/conda/lib/python3.8/site-packages (from ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (7.0.0)\n",
      "Collecting tensorflow==2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp38-cp38-manylinux2010_x86_64.whl (320.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 320.5 MB 12.9 MB/s eta 0:00:01    |█▍                              | 13.6 MB 16.9 MB/s eta 0:00:19     |████▍                           | 43.8 MB 11.9 MB/s eta 0:00:24     |████▍                           | 44.3 MB 11.9 MB/s eta 0:00:24     |█████▌                          | 55.2 MB 7.8 MB/s eta 0:00:34     |██████████████                  | 140.2 MB 11.1 MB/s eta 0:00:17     |██████████████▏                 | 142.4 MB 11.1 MB/s eta 0:00:17     |██████████████▊                 | 147.6 MB 11.5 MB/s eta 0:00:16     |███████████████▌                | 154.9 MB 10.1 MB/s eta 0:00:17     |██████████████████▏             | 182.0 MB 9.5 MB/s eta 0:00:15     |███████████████████▎            | 192.8 MB 10.9 MB/s eta 0:00:12     |████████████████████▌           | 205.4 MB 11.6 MB/s eta 0:00:10     |██████████████████████▋         | 226.2 MB 10.9 MB/s eta 0:00:09     |███████████████████████▉        | 239.0 MB 4.7 MB/s eta 0:00:18     |████████████████████████        | 240.3 MB 4.7 MB/s eta 0:00:18     |██████████████████████████▎     | 263.6 MB 15.0 MB/s eta 0:00:04     |██████████████████████████▋     | 266.9 MB 2.1 MB/s eta 0:00:26     |████████████████████████████▌   | 285.5 MB 1.6 MB/s eta 0:00:22     |███████████████████████████████ | 311.3 MB 11.2 MB/s eta 0:00:01     |███████████████████████████████▋| 316.9 MB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (0.36.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (1.15.0)\n",
      "Collecting zipp==3.1.0\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 7.0 MB/s eta 0:00:011     |█████████████████████████████▋  | 24.0 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 9.3 MB/s eta 0:00:011    |██████████▎                     | 6.6 MB 11.4 MB/s eta 0:00:02     |███████████▎                    | 7.2 MB 11.4 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.34.0-cp38-cp38-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from keras<=2.4.3->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (5.3.1)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (1.4.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.5.1)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (2.11.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.6.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (5.0.5)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (4.7.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (2.7.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (3.2.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.4.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (1.1.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /opt/conda/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (6.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (1.4.3)\n",
      "Requirement already satisfied: async-generator in /opt/conda/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (1.10)\n",
      "Requirement already satisfied: tornado>=4.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (6.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (20.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (2.8.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat<=5.0.8->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat<=5.0.8->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat<=5.0.8->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (51.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat<=5.0.8->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat<=5.0.8->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (20.3.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 9.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (2.25.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (2.10)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0->-r /workdir/root/develop/requirements.txt (line 9)) (3.0.1)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting clint\n",
      "  Downloading clint-0.5.1.tar.gz (29 kB)\n",
      "Collecting crontab\n",
      "  Downloading crontab-0.22.9.tar.gz (20 kB)\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.21-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython->-r /workdir/root/develop/requirements.txt (line 14)) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython->-r /workdir/root/develop/requirements.txt (line 14)) (3.0.8)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.8/site-packages (from ipython->-r /workdir/root/develop/requirements.txt (line 14)) (0.17.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython->-r /workdir/root/develop/requirements.txt (line 14)) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython->-r /workdir/root/develop/requirements.txt (line 14)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython->-r /workdir/root/develop/requirements.txt (line 14)) (0.7.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.10->ipython->-r /workdir/root/develop/requirements.txt (line 14)) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython->-r /workdir/root/develop/requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r /workdir/root/develop/requirements.txt (line 14)) (0.2.5)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.3.3-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 9.4 MB/s eta 0:00:011    |███████████████▌                | 5.6 MB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->-r /workdir/root/develop/requirements.txt (line 6)) (2.4.7)\n",
      "Collecting Pillow\n",
      "  Downloading Pillow-8.0.1-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp38-cp38-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.3 MB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Collecting tablib\n",
      "  Downloading tablib-3.0.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 24.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tf_slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[K     |████████████████████████████████| 352 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting args\n",
      "  Downloading args-0.1.0.tar.gz (3.0 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (20.8)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert<=6.0.7->ait-sdk==0.1.5->-r /workdir/root/develop/requirements.txt (line 16)) (0.5.1)\n",
      "ait-sdk is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Building wheels for collected packages: termcolor, wrapt, clint, crontab, docopt, pycocotools, args\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=a956016394708d8baff81f40b180d1c98a99b6303da7bf9b789a81b5f1c88d81\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ffs5wd_y/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=81776 sha256=83a2297fa5efaf5f9ba8fa4a38a6dc3a8d4bb58ae7725d390fdf7dcd0d4b8cb2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ffs5wd_y/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for clint (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clint: filename=clint-0.5.1-py3-none-any.whl size=34448 sha256=65f5f028b08a5430615e37cfc5173afaa4c0883c80402506b285998c2105994a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ffs5wd_y/wheels/84/c4/6d/3235262e25e25ce8f080be07d9181853c92e0be64bb1a5d219\n",
      "  Building wheel for crontab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crontab: filename=crontab-0.22.9-py3-none-any.whl size=20496 sha256=89aeeb3a93681978bac29d15895d2bc967761b9e8ead3a16e9d1b330b7192bf8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ffs5wd_y/wheels/74/48/f8/ef8c7d6574b9df6f669e30625b203cdbee7d6d41399d5d1379\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=24f69287b88114d8fb88662cc3d708e06b65000a4a0235b0b1920a4de21cbe53\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ffs5wd_y/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp38-cp38-linux_x86_64.whl size=421101 sha256=918ea95ad7ac3495e96d35c9cb5aa3038a3e08fd52d010a48e71539d9a0906b1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ffs5wd_y/wheels/e7/77/b2/6f38b5bea571cd8f4689f91a7c1ed2eaecb2c2ce17f9945b17\n",
      "  Building wheel for args (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for args: filename=args-0.1.0-py3-none-any.whl size=3318 sha256=fc4b2565d004d430c3fe1fe05c2489915ec8a506216869258170d07a1986d24d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ffs5wd_y/wheels/08/5b/6a/d4c08f9d355023389016ed57db4099cf7091627fd5de717da6\n",
      "Successfully built termcolor wrapt clint crontab docopt pycocotools args\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, numpy, google-auth, werkzeug, tensorboard-plugin-wit, scipy, protobuf, Pillow, markdown, kiwisolver, h5py, grpcio, google-auth-oauthlib, cycler, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, pytz, opt-einsum, matplotlib, keras-preprocessing, google-pasta, gast, Cython, astunparse, args, zipp, tf-slim, tensorflow, tablib, pycocotools, pandas, lxml, docopt, crontab, clint\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.3\n",
      "    Uninstalling numpy-1.19.3:\n",
      "      Successfully uninstalled numpy-1.19.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.4\n",
      "    Uninstalling scipy-1.5.4:\n",
      "      Successfully uninstalled scipy-1.5.4\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.4.0\n",
      "    Uninstalling zipp-3.4.0:\n",
      "      Successfully uninstalled zipp-3.4.0\n",
      "Successfully installed Cython-0.29.21 Pillow-8.0.1 absl-py-0.11.0 args-0.1.0 astunparse-1.6.3 cachetools-4.2.0 clint-0.5.1 crontab-0.22.9 cycler-0.10.0 docopt-0.6.2 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.34.0 h5py-2.10.0 keras-preprocessing-1.1.2 kiwisolver-1.3.1 lxml-4.6.2 markdown-3.3.3 matplotlib-3.3.3 numpy-1.18.5 opt-einsum-3.3.0 pandas-1.1.5 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycocotools-2.0.2 pytz-2020.4 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tablib-3.0.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0 tf-slim-1.1.0 werkzeug-1.0.1 wrapt-1.12.1 zipp-3.1.0\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# area:create requirements and pip install\n",
    "# do not edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    requirements_generator.add_package(f'./{ait_sdk_name}')\n",
    "    requirements_path = requirements_generator.create_requirements(current_dir)\n",
    "\n",
    "    !pip install -r $requirements_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:import\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "# import if you need modules cell\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pathlib\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from os import makedirs, path\n",
    "from collections import Iterable\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:import\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "# must use modules\n",
    "import shutil  # do not remove\n",
    "from ait_sdk.common.files.ait_input import AITInput  # do not remove\n",
    "from ait_sdk.common.files.ait_output import AITOutput  # do not remove\n",
    "from ait_sdk.common.files.ait_manifest import AITManifest  # do not remove\n",
    "from ait_sdk.develop.ait_path_helper import AITPathHelper  # do not remove\n",
    "from ait_sdk.utils.logging import get_logger, log, get_log_path  # do not remove\n",
    "from ait_sdk.develop.annotation import measures, resources, downloads, ait_main  # do not remove\n",
    "# must use modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create manifest\n",
    "# should edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_manifest_generator import AITManifestGenerator\n",
    "    \n",
    "    manifest_genenerator = AITManifestGenerator(current_dir)\n",
    "    manifest_genenerator.set_ait_name('eval_bdd100k_aicc_tf2.3')\n",
    "    manifest_genenerator.set_ait_description('''The image classification model infers the image data (.jpg).\n",
    "Compare the inference result with the correct answer data (.json).\n",
    "Output the coverage of the comparison result.\n",
    "!!!Caution!!!\n",
    "Please set the memory allocation of docker to 4GB or more.''')\n",
    "    manifest_genenerator.set_ait_author('AIST')\n",
    "    manifest_genenerator.set_ait_email('')\n",
    "    manifest_genenerator.set_ait_version('0.1')\n",
    "    manifest_genenerator.set_ait_quality('https://airc.aist.go.jp/aiqm/quality/internal/Accuracy_of_trained_model')\n",
    "    manifest_genenerator.set_ait_reference('')\n",
    "    manifest_genenerator.add_ait_inventories(name='trained_model_checkpoint', \n",
    "                                             type_='model', \n",
    "                                             description='trained_model_checkpoint', \n",
    "                                             format_=['zip'], \n",
    "                                             schema='https://www.tensorflow.org/guide/saved_model')\n",
    "    manifest_genenerator.add_ait_inventories(name='trained_model_graph', \n",
    "                                             type_='model', \n",
    "                                             description='trained_model_graph', \n",
    "                                             format_=['zip'], \n",
    "                                             schema='https://www.tensorflow.org/guide/saved_model')\n",
    "    manifest_genenerator.add_ait_inventories(name='trained_model_protobuf', \n",
    "                                             type_='model', \n",
    "                                             description='trained_model_protobuf', \n",
    "                                             format_=['zip'], \n",
    "                                             schema='https://www.tensorflow.org/guide/saved_model')\n",
    "    manifest_genenerator.add_ait_inventories(name='test_set_images', \n",
    "                                             type_='dataset', \n",
    "                                             description='image_dataset（bdd100K）', \n",
    "                                             format_=['zip'], \n",
    "                                             schema='https://bdd-data.berkeley.edu/')\n",
    "    manifest_genenerator.add_ait_inventories(name='test_set_labels', \n",
    "                                             type_='dataset', \n",
    "                                             description='image_label_dataset（bdd100K）', \n",
    "                                             format_=['json'], \n",
    "                                             schema='https://bdd-data.berkeley.edu/')\n",
    "    manifest_genenerator.add_ait_inventories(name='labels_define', \n",
    "                                             type_='dataset', \n",
    "                                             description='labels_define', \n",
    "                                             format_=['txt'], \n",
    "                                             schema='https://github.com/tensorflow/models/tree/master/research/object_detection/data')\n",
    "    manifest_genenerator.add_ait_measures(name='traffic_sign_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of traffic_sign', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='traffic_light_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of traffic_light', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='car_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of car', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='rider_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of rider', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='motor_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of motor', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='person_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of person', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='bus_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of bus', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='truck_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of truck', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='bike_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of bike', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='train_accuracy', \n",
    "                                          type_='float', \n",
    "                                          description='accuracy predicted of train', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_resources(name='all_label_accuracy_csv',  \n",
    "                                           type_='text', \n",
    "                                           description='accuracy of all label')\n",
    "    manifest_genenerator.add_ait_resources(name='all_label_accuracy_png', \n",
    "                                           type_='picture', \n",
    "                                           description='accuracy of all label')\n",
    "    manifest_genenerator.add_ait_downloads(name='Log', \n",
    "                                           description='AIT_log')\n",
    "    manifest_genenerator.add_ait_downloads(name='each_label_accuracy', \n",
    "                                           description='accuracy of each label')\n",
    "    manifest_genenerator.add_ait_downloads(name='each_picture', \n",
    "                                           description='predict of each picture')\n",
    "    manifest_path = manifest_genenerator.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create input\n",
    "# should edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_input_generator import AITInputGenerator\n",
    "    input_generator = AITInputGenerator(manifest_path)\n",
    "    input_generator.add_ait_inventories(name='trained_model_checkpoint',\n",
    "                                        value='trained_model_checkpoint/trained_model_checkpoint.zip')\n",
    "    input_generator.add_ait_inventories(name='trained_model_graph',\n",
    "                                        value='trained_model_graph/trained_model_graph.zip')\n",
    "    input_generator.add_ait_inventories(name='trained_model_protobuf',\n",
    "                                        value='trained_model_protobuf/trained_model_protobuf.zip')\n",
    "    input_generator.add_ait_inventories(name='test_set_images',\n",
    "                                        value='test_set_images/test_set_images.zip')\n",
    "    input_generator.add_ait_inventories(name='test_set_labels',\n",
    "                                        value='test_set_labels/bdd100k_labels_images_val.json')\n",
    "    input_generator.add_ait_inventories(name='labels_define',\n",
    "                                        value='labels_define/mscoco_complete_label_map.pbtxt')\n",
    "    input_generator.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:initialize\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "ait_manifest = AITManifest()\n",
    "ait_input = AITInput(ait_manifest)\n",
    "ait_output = AITOutput(ait_manifest)\n",
    "\n",
    "if is_ait_launch:\n",
    "    # launch from AIT\n",
    "    current_dir = path.dirname(path.abspath(__file__))\n",
    "    path_helper = AITPathHelper(argv=sys.argv, ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "else:\n",
    "    # launch from jupyter notebook\n",
    "    # ait.input.json make in input_dir\n",
    "    input_dir = '/usr/local/qai/mnt/ip/job_args/1/1'\n",
    "    current_dir = %pwd\n",
    "    path_helper = AITPathHelper(argv=['', input_dir], ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "\n",
    "ait_input.read_json(path_helper.get_input_file_path())\n",
    "ait_manifest.read_json(path_helper.get_manifest_file_path())\n",
    "\n",
    "### do not edit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "(Reading database ... 61330 files and directories currently installed.)\n",
      "Preparing to unpack .../libssl1.1_1.1.1f-1ubuntu2.1_amd64.deb ...\n",
      "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2.1) over (1.1.1f-1ubuntu2) ...\n",
      "Selecting previously unselected package libssl-dev:amd64.\n",
      "Preparing to unpack .../libssl-dev_1.1.1f-1ubuntu2.1_amd64.deb ...\n",
      "Unpacking libssl-dev:amd64 (1.1.1f-1ubuntu2.1) ...\n",
      "Selecting previously unselected package libffi-dev:amd64.\n",
      "Preparing to unpack .../libffi-dev_3.3-4_amd64.deb ...\n",
      "Unpacking libffi-dev:amd64 (3.3-4) ...\n",
      "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2.1) ...\n",
      "Setting up libffi-dev:amd64 (3.3-4) ...\n",
      "Setting up libssl-dev:amd64 (1.1.1f-1ubuntu2.1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.1) ...\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  python-backports.functools-lru-cache python-bs4 python-chardet\n",
      "  python-html5lib python-pkg-resources python-six python-soupsieve\n",
      "  python-webencodings\n",
      "Suggested packages:\n",
      "  python-genshi python-lxml-dbg python-lxml-doc python-setuptools\n",
      "The following NEW packages will be installed:\n",
      "  python-backports.functools-lru-cache python-bs4 python-chardet\n",
      "  python-html5lib python-lxml python-pkg-resources python-six python-soupsieve\n",
      "  python-webencodings\n",
      "0 upgraded, 9 newly installed, 0 to remove and 13 not upgraded.\n",
      "Need to get 1,388 kB of archives.\n",
      "After this operation, 8,063 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-backports.functools-lru-cache all 1.5-3build1 [6,520 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-soupsieve all 1.9.5+dfsg-1 [29.0 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-bs4 all 4.8.2-1 [83.2 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-pkg-resources all 44.0.0-2 [129 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-chardet all 3.0.4-4build1 [80.5 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-six all 1.14.0-2 [12.0 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-webencodings all 0.5.1-1ubuntu1 [10.9 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-html5lib all 1.0.1-2 [84.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-lxml amd64 4.5.0-1ubuntu0.2 [951 kB]\n",
      "Fetched 1,388 kB in 6s (251 kB/s)    \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package python-backports.functools-lru-cache.\n",
      "(Reading database ... 61481 files and directories currently installed.)\n",
      "Preparing to unpack .../0-python-backports.functools-lru-cache_1.5-3build1_all.deb ...\n",
      "Unpacking python-backports.functools-lru-cache (1.5-3build1) ...\n",
      "Selecting previously unselected package python-soupsieve.\n",
      "Preparing to unpack .../1-python-soupsieve_1.9.5+dfsg-1_all.deb ...\n",
      "Unpacking python-soupsieve (1.9.5+dfsg-1) ...\n",
      "Selecting previously unselected package python-bs4.\n",
      "Preparing to unpack .../2-python-bs4_4.8.2-1_all.deb ...\n",
      "Unpacking python-bs4 (4.8.2-1) ...\n",
      "Selecting previously unselected package python-pkg-resources.\n",
      "Preparing to unpack .../3-python-pkg-resources_44.0.0-2_all.deb ...\n",
      "Unpacking python-pkg-resources (44.0.0-2) ...\n",
      "Selecting previously unselected package python-chardet.\n",
      "Preparing to unpack .../4-python-chardet_3.0.4-4build1_all.deb ...\n",
      "Unpacking python-chardet (3.0.4-4build1) ...\n",
      "Selecting previously unselected package python-six.\n",
      "Preparing to unpack .../5-python-six_1.14.0-2_all.deb ...\n",
      "Unpacking python-six (1.14.0-2) ...\n",
      "Selecting previously unselected package python-webencodings.\n",
      "Preparing to unpack .../6-python-webencodings_0.5.1-1ubuntu1_all.deb ...\n",
      "Unpacking python-webencodings (0.5.1-1ubuntu1) ...\n",
      "Selecting previously unselected package python-html5lib.\n",
      "Preparing to unpack .../7-python-html5lib_1.0.1-2_all.deb ...\n",
      "Unpacking python-html5lib (1.0.1-2) ...\n",
      "Selecting previously unselected package python-lxml:amd64.\n",
      "Preparing to unpack .../8-python-lxml_4.5.0-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking python-lxml:amd64 (4.5.0-1ubuntu0.2) ...\n",
      "Setting up python-pkg-resources (44.0.0-2) ...\n",
      "Setting up python-six (1.14.0-2) ...\n",
      "Setting up python-backports.functools-lru-cache (1.5-3build1) ...\n",
      "Setting up python-chardet (3.0.4-4build1) ...\n",
      "Setting up python-webencodings (0.5.1-1ubuntu1) ...\n",
      "Setting up python-lxml:amd64 (4.5.0-1ubuntu0.2) ...\n",
      "Setting up python-html5lib (1.0.1-2) ...\n",
      "Setting up python-soupsieve (1.9.5+dfsg-1) ...\n",
      "Setting up python-bs4 (4.8.2-1) ...\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Note, selecting 'python-dev-is-python2' instead of 'python-dev'\n",
      "Note, selecting 'libxslt1-dev' instead of 'libxslt-dev'\n",
      "libxml2 is already the newest version (2.9.10+dfsg-5).\n",
      "libxml2 set to manually installed.\n",
      "curl is already the newest version (7.68.0-1ubuntu2.4).\n",
      "The following additional packages will be installed:\n",
      "  icu-devtools libexpat1-dev libicu-dev libpython2-dev libpython2.7\n",
      "  libpython2.7-dev python-is-python2 python2-dev python2.7-dev\n",
      "Suggested packages:\n",
      "  icu-doc pkg-config\n",
      "The following NEW packages will be installed:\n",
      "  icu-devtools libexpat1-dev libicu-dev libpython2-dev libpython2.7\n",
      "  libpython2.7-dev libxml2-dev libxslt1-dev python-dev-is-python2\n",
      "  python-is-python2 python2-dev python2.7-dev\n",
      "0 upgraded, 12 newly installed, 0 to remove and 13 not upgraded.\n",
      "Need to get 14.5 MB of archives.\n",
      "After this operation, 69.5 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 icu-devtools amd64 66.1-2ubuntu2 [188 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libexpat1-dev amd64 2.2.9-1build1 [116 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libicu-dev amd64 66.1-2ubuntu2 [9,450 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7 amd64 2.7.18-1~20.04 [1,036 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-dev amd64 2.7.18-1~20.04 [2,476 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-dev amd64 2.7.17-2ubuntu4 [7,140 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libxml2-dev amd64 2.9.10+dfsg-5 [737 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libxslt1-dev amd64 1.1.34-4 [219 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-is-python2 all 2.7.17-4 [2,496 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7-dev amd64 2.7.18-1~20.04 [287 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-dev amd64 2.7.17-2ubuntu4 [1,268 B]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-dev-is-python2 all 2.7.17-4 [1,396 B]\n",
      "Fetched 14.5 MB in 52s (279 kB/s)                                              \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package icu-devtools.\n",
      "(Reading database ... 61774 files and directories currently installed.)\n",
      "Preparing to unpack .../00-icu-devtools_66.1-2ubuntu2_amd64.deb ...\n",
      "Unpacking icu-devtools (66.1-2ubuntu2) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../01-libexpat1-dev_2.2.9-1build1_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.2.9-1build1) ...\n",
      "Selecting previously unselected package libicu-dev:amd64.\n",
      "Preparing to unpack .../02-libicu-dev_66.1-2ubuntu2_amd64.deb ...\n",
      "Unpacking libicu-dev:amd64 (66.1-2ubuntu2) ...\n",
      "Selecting previously unselected package libpython2.7:amd64.\n",
      "Preparing to unpack .../03-libpython2.7_2.7.18-1~20.04_amd64.deb ...\n",
      "Unpacking libpython2.7:amd64 (2.7.18-1~20.04) ...\n",
      "Selecting previously unselected package libpython2.7-dev:amd64.\n",
      "Preparing to unpack .../04-libpython2.7-dev_2.7.18-1~20.04_amd64.deb ...\n",
      "Unpacking libpython2.7-dev:amd64 (2.7.18-1~20.04) ...\n",
      "Selecting previously unselected package libpython2-dev:amd64.\n",
      "Preparing to unpack .../05-libpython2-dev_2.7.17-2ubuntu4_amd64.deb ...\n",
      "Unpacking libpython2-dev:amd64 (2.7.17-2ubuntu4) ...\n",
      "Selecting previously unselected package libxml2-dev:amd64.\n",
      "Preparing to unpack .../06-libxml2-dev_2.9.10+dfsg-5_amd64.deb ...\n",
      "Unpacking libxml2-dev:amd64 (2.9.10+dfsg-5) ...\n",
      "Selecting previously unselected package libxslt1-dev:amd64.\n",
      "Preparing to unpack .../07-libxslt1-dev_1.1.34-4_amd64.deb ...\n",
      "Unpacking libxslt1-dev:amd64 (1.1.34-4) ...\n",
      "Selecting previously unselected package python-is-python2.\n",
      "Preparing to unpack .../08-python-is-python2_2.7.17-4_all.deb ...\n",
      "Unpacking python-is-python2 (2.7.17-4) ...\n",
      "Selecting previously unselected package python2.7-dev.\n",
      "Preparing to unpack .../09-python2.7-dev_2.7.18-1~20.04_amd64.deb ...\n",
      "Unpacking python2.7-dev (2.7.18-1~20.04) ...\n",
      "Selecting previously unselected package python2-dev.\n",
      "Preparing to unpack .../10-python2-dev_2.7.17-2ubuntu4_amd64.deb ...\n",
      "Unpacking python2-dev (2.7.17-2ubuntu4) ...\n",
      "Selecting previously unselected package python-dev-is-python2.\n",
      "Preparing to unpack .../11-python-dev-is-python2_2.7.17-4_all.deb ...\n",
      "Unpacking python-dev-is-python2 (2.7.17-4) ...\n",
      "Setting up libpython2.7:amd64 (2.7.18-1~20.04) ...\n",
      "Setting up libexpat1-dev:amd64 (2.2.9-1build1) ...\n",
      "Setting up icu-devtools (66.1-2ubuntu2) ...\n",
      "Setting up libicu-dev:amd64 (66.1-2ubuntu2) ...\n",
      "Setting up python-is-python2 (2.7.17-4) ...\n",
      "Setting up libpython2.7-dev:amd64 (2.7.18-1~20.04) ...\n",
      "Setting up libxml2-dev:amd64 (2.9.10+dfsg-5) ...\n",
      "Setting up libpython2-dev:amd64 (2.7.17-2ubuntu4) ...\n",
      "Setting up python2.7-dev (2.7.18-1~20.04) ...\n",
      "Setting up python2-dev (2.7.17-2ubuntu4) ...\n",
      "Setting up libxslt1-dev:amd64 (1.1.34-4) ...\n",
      "Setting up python-dev-is-python2 (2.7.17-4) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.1) ...\n",
      "/tmp\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-25ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   164  100   164    0     0    495      0 --:--:-- --:--:-- --:--:--   495\n",
      "100   653  100   653    0     0   1058      0 --:--:-- --:--:-- --:--:--  637k\n",
      "100 1519k  100 1519k    0     0   523k      0  0:00:02  0:00:02 --:--:-- 1185k\n",
      "Archive:  protoc-3.9.0-linux-x86_64.zip\n",
      "   creating: protoc3/include/\n",
      "   creating: protoc3/include/google/\n",
      "   creating: protoc3/include/google/protobuf/\n",
      "  inflating: protoc3/include/google/protobuf/empty.proto  \n",
      "  inflating: protoc3/include/google/protobuf/duration.proto  \n",
      "  inflating: protoc3/include/google/protobuf/timestamp.proto  \n",
      "  inflating: protoc3/include/google/protobuf/api.proto  \n",
      "  inflating: protoc3/include/google/protobuf/type.proto  \n",
      "  inflating: protoc3/include/google/protobuf/any.proto  \n",
      "  inflating: protoc3/include/google/protobuf/field_mask.proto  \n",
      "  inflating: protoc3/include/google/protobuf/descriptor.proto  \n",
      "  inflating: protoc3/include/google/protobuf/wrappers.proto  \n",
      "  inflating: protoc3/include/google/protobuf/struct.proto  \n",
      "   creating: protoc3/include/google/protobuf/compiler/\n",
      "  inflating: protoc3/include/google/protobuf/compiler/plugin.proto  \n",
      "  inflating: protoc3/include/google/protobuf/source_context.proto  \n",
      "   creating: protoc3/bin/\n",
      "  inflating: protoc3/bin/protoc      \n",
      "  inflating: protoc3/readme.txt      \n"
     ]
    }
   ],
   "source": [
    "if not is_ait_launch:\n",
    "    \n",
    "    !apt-get update -yqq\n",
    "    !apt-get install gcc g++ make libffi-dev libssl-dev -yqq\n",
    "    !apt-get install python-lxml -y\n",
    "    !apt-get install python-dev libxml2 libxml2-dev libxslt-dev curl -y\n",
    "\n",
    "    !export PYTHONPATH=$PYTHONPATH:/tensorflow/models/research:/tensorflow/\n",
    "\n",
    "    %cd /tmp\n",
    "    !apt-get install -y unzip\n",
    "    !curl -OL https://github.com/google/protobuf/releases/download/v3.9.0/protoc-3.9.0-linux-x86_64.zip\n",
    "    !unzip -d protoc3 protoc-3.9.0-linux-x86_64.zip\n",
    "    !mv protoc3/bin/* /usr/local/bin/\n",
    "    !mv protoc3/include/* /usr/local/include/\n",
    "    !rm -rf protoc-3.9.0-linux-x86_64.zip protoc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.25.1-1ubuntu3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
      "Cloning into 'cocoapi'...\n",
      "remote: Enumerating objects: 975, done.\u001b[K\n",
      "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
      "Receiving objects: 100% (975/975), 11.72 MiB | 5.74 MiB/s, done.\n",
      "Resolving deltas: 100% (576/576), done.\n",
      "/tmp/cocoapi/PythonAPI\n",
      "running build_ext\n",
      "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "/opt/conda/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.8\n",
      "creating build/temp.linux-x86_64-3.8/pycocotools\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/numpy/core/include -I../common -I/opt/conda/include/python3.8 -c ../common/maskApi.c -o build/temp.linux-x86_64-3.8/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   46 |       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "      |       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   46 |       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "      |                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  166 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  166 |   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  167 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  167 |   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  212 |       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "      |       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  212 |       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "      |                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  220 |   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "  220 |   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "      |                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  228 |     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "      |     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  228 |     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "      |                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/numpy/core/include -I../common -I/opt/conda/include/python3.8 -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.8/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-3.8\n",
      "creating build/lib.linux-x86_64-3.8/pycocotools\n",
      "gcc -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.8/../common/maskApi.o build/temp.linux-x86_64-3.8/pycocotools/_mask.o -o build/lib.linux-x86_64-3.8/pycocotools/_mask.cpython-38-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.8/pycocotools/_mask.cpython-38-x86_64-linux-gnu.so -> pycocotools\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.8\n",
      "creating build/temp.linux-x86_64-3.8/pycocotools\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/numpy/core/include -I../common -I/opt/conda/include/python3.8 -c ../common/maskApi.c -o build/temp.linux-x86_64-3.8/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   46 |       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "      |       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   46 |       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "      |                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  166 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  166 |   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  167 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  167 |   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  212 |       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "      |       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  212 |       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "      |                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  220 |   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "  220 |   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "      |                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "  228 |     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "      |     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  228 |     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "      |                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/numpy/core/include -I../common -I/opt/conda/include/python3.8 -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.8/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-3.8\n",
      "creating build/lib.linux-x86_64-3.8/pycocotools\n",
      "gcc -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.8/../common/maskApi.o build/temp.linux-x86_64-3.8/pycocotools/_mask.o -o build/lib.linux-x86_64-3.8/pycocotools/_mask.cpython-38-x86_64-linux-gnu.so\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating pycocotools.egg-info\n",
      "writing pycocotools.egg-info/PKG-INFO\n",
      "writing dependency_links to pycocotools.egg-info/dependency_links.txt\n",
      "writing requirements to pycocotools.egg-info/requires.txt\n",
      "writing top-level names to pycocotools.egg-info/top_level.txt\n",
      "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "reading manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "copying pycocotools/coco.py -> build/lib.linux-x86_64-3.8/pycocotools\n",
      "copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.8/pycocotools\n",
      "copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.8/pycocotools\n",
      "copying pycocotools/mask.py -> build/lib.linux-x86_64-3.8/pycocotools\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.8/pycocotools/coco.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.8/pycocotools/__init__.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.8/pycocotools/cocoeval.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.8/pycocotools/_mask.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.8/pycocotools/mask.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/coco.py to coco.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/__init__.py to __init__.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/mask.py to mask.cpython-38.pyc\n",
      "creating stub loader for pycocotools/_mask.cpython-38-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/_mask.py to _mask.cpython-38.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "pycocotools.__pycache__._mask.cpython-38: module references __file__\n",
      "creating dist\n",
      "creating 'dist/pycocotools-2.0-py3.8-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing pycocotools-2.0-py3.8-linux-x86_64.egg\n",
      "creating /opt/conda/lib/python3.8/site-packages/pycocotools-2.0-py3.8-linux-x86_64.egg\n",
      "Extracting pycocotools-2.0-py3.8-linux-x86_64.egg to /opt/conda/lib/python3.8/site-packages\n",
      "Adding pycocotools 2.0 to easy-install.pth file\n",
      "\n",
      "Installed /opt/conda/lib/python3.8/site-packages/pycocotools-2.0-py3.8-linux-x86_64.egg\n",
      "Processing dependencies for pycocotools==2.0\n",
      "Searching for matplotlib==3.3.3\n",
      "Best match: matplotlib 3.3.3\n",
      "Adding matplotlib 3.3.3 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for Cython==0.29.21\n",
      "Best match: Cython 0.29.21\n",
      "Adding Cython 0.29.21 to easy-install.pth file\n",
      "Installing cygdb script to /opt/conda/bin\n",
      "Installing cython script to /opt/conda/bin\n",
      "Installing cythonize script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for setuptools==51.1.0\n",
      "Best match: setuptools 51.1.0\n",
      "Adding setuptools 51.1.0 to easy-install.pth file\n",
      "Installing easy_install script to /opt/conda/bin\n",
      "Installing easy_install-3.9 script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for pyparsing==2.4.7\n",
      "Best match: pyparsing 2.4.7\n",
      "Adding pyparsing 2.4.7 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for numpy==1.18.5\n",
      "Best match: numpy 1.18.5\n",
      "Adding numpy 1.18.5 to easy-install.pth file\n",
      "Installing f2py script to /opt/conda/bin\n",
      "Installing f2py3 script to /opt/conda/bin\n",
      "Installing f2py3.8 script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for cycler==0.10.0\n",
      "Best match: cycler 0.10.0\n",
      "Adding cycler 0.10.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for kiwisolver==1.3.1\n",
      "Best match: kiwisolver 1.3.1\n",
      "Adding kiwisolver 1.3.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for python-dateutil==2.8.1\n",
      "Best match: python-dateutil 2.8.1\n",
      "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for Pillow==8.0.1\n",
      "Best match: Pillow 8.0.1\n",
      "Adding Pillow 8.0.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for six==1.15.0\n",
      "Best match: six 1.15.0\n",
      "Adding six 1.15.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Finished processing dependencies for pycocotools==2.0\n",
      "/tensorflow\n",
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 47, done.\u001b[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
      "remote: Total 49553 (delta 24), reused 23 (delta 0), pack-reused 49506\u001b[K\n",
      "Receiving objects: 100% (49553/49553), 558.67 MiB | 10.44 MiB/s, done.\n",
      "Resolving deltas: 100% (34169/34169), done.\n",
      "/tensorflow/models/research\n",
      "/workdir/root/develop\n"
     ]
    }
   ],
   "source": [
    "if not is_ait_launch:\n",
    "    !apt-get install git -y\n",
    "    !git clone https://github.com/cocodataset/cocoapi.git\n",
    "    %cd /tmp/cocoapi/PythonAPI\n",
    "    !python3 setup.py build_ext --inplace\n",
    "    !rm -rf build\n",
    "    !python3 setup.py build_ext install\n",
    "    !rm -rf build\n",
    "\n",
    "    !mkdir /tensorflow\n",
    "    %cd /tensorflow\n",
    "    !git clone https://github.com/tensorflow/models.git\n",
    "    %cd /tensorflow/models/research\n",
    "    !protoc object_detection/protos/*.proto --python_out=.\n",
    "    \n",
    "    %cd /workdir/root/develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(Path().resolve(), '/tensorflow/models/research'))\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "utils_ops.tf = tf.compat.v1\n",
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "g_total_accuracy = []\n",
    "\n",
    "resultFile = []\n",
    "# With a range of -1 you will get evrything\n",
    "limit_box_range = 0\n",
    "# Obtaining yolo results\n",
    "g_image_name = \"\"\n",
    "count_traffic_label = 0\n",
    "gt_list_boxes = []\n",
    "\n",
    "total_correct_predicted = [0,0,0,0,0,0,0,0,0,0]\n",
    "total_GT_correct_predicted = [0,0,0,0,0,0,0,0,0,0]\n",
    "total_predicted_per_label = [0,0,0,0,0,0,0,0,0,0]\n",
    "correct_predicted = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "jsonFileName = \"\"\n",
    "\n",
    "average_accuracy = []\n",
    "avg_acc_per_traffic_sign = []\n",
    "avg_acc_per_traffic_light = []\n",
    "avg_acc_per_car = []\n",
    "avg_acc_per_rider = []\n",
    "avg_acc_per_motor = []\n",
    "avg_acc_per_person = []\n",
    "avg_acc_per_bus = []\n",
    "avg_acc_per_truck = []\n",
    "avg_acc_per_bike = []\n",
    "avg_acc_per_train = []\n",
    "\n",
    "f_each = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_inference_for_single_image(model, image) -> np.ndarray:\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                                for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "                            image.shape[0], image.shape[1])\t\t\t\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_labels(label_path) -> np.ndarray:\n",
    "    labels = json.load(open(label_path, 'r'))\n",
    "    if not isinstance(labels, Iterable):\n",
    "        labels = [labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_boxes(objects) -> np.ndarray:\n",
    "    return [o for o in objects if ('box2d' in o and o['box2d'] is not None)\n",
    "            or ('box3d' in o and o['box3d'] is not None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_id_label(current_object_label) -> int:\n",
    "    if current_object_label == \"traffic sign\":\n",
    "        #traffic_sign = 0\n",
    "        return 0\n",
    "    elif current_object_label == \"traffic light\":\n",
    "        #traffic light = 1\n",
    "        return 1\n",
    "    elif current_object_label == \"car\":\n",
    "        #car = 2\n",
    "        return 2\n",
    "    elif current_object_label == \"rider\":\n",
    "        #rider = 3\n",
    "        return 3\n",
    "    elif current_object_label == \"motor\":\n",
    "        #motor = 4\n",
    "        return 4\n",
    "    elif current_object_label == \"person\":\n",
    "        #person = 5\n",
    "        return 5\n",
    "    elif current_object_label == \"bus\":\n",
    "        #bus = 6\n",
    "        return 6\n",
    "    elif current_object_label == \"truck\":\n",
    "        #truck = 7\n",
    "        return 7\n",
    "    elif current_object_label == \"bike\":\n",
    "        #bike = 8\n",
    "        return 8\n",
    "    elif current_object_label == \"train\":\n",
    "        #train = 9\n",
    "        return 9\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_label_from_id(id) -> str:\n",
    "    if id == 0:\n",
    "        return \"traffic sign\"\n",
    "    elif id == 1:\n",
    "        return \"traffic light\"\n",
    "    elif id == 2:\n",
    "        return \"car\"\n",
    "    elif id == 3:\n",
    "        return \"rider\"\n",
    "    elif id == 4:\n",
    "        return \"motor\"\n",
    "    elif id == 5:\n",
    "        return \"person\"\n",
    "    elif id == 6:\n",
    "        return \"bus\"\n",
    "    elif id == 7:\n",
    "        return \"truck\"\n",
    "    elif id == 8:\n",
    "        return \"bike\"\n",
    "    elif id == 9:\n",
    "        return \"train\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adding_avg_label_from_id(id, value, avg_acc_per_traffic_sign, avg_acc_per_traffic_light, \n",
    "                                avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, \n",
    "                                avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train):\n",
    "    if id == 0:\n",
    "        avg_acc_per_traffic_sign.append(value)\n",
    "    elif id == 1:\n",
    "        avg_acc_per_traffic_light.append(value)\n",
    "    elif id == 2:\n",
    "        avg_acc_per_car.append(value)\n",
    "    elif id == 3:\n",
    "        avg_acc_per_rider.append(value)\n",
    "    elif id == 4:\n",
    "        avg_acc_per_motor.append(value)\n",
    "    elif id == 5:\n",
    "        avg_acc_per_person.append(value)\n",
    "    elif id == 6:\n",
    "        avg_acc_per_bus.append(value)\n",
    "    elif id == 7:\n",
    "        avg_acc_per_truck.append(value)\n",
    "    elif id == 8:\n",
    "        avg_acc_per_bike.append(value)\n",
    "    elif id == 9:\n",
    "        avg_acc_per_train.append(value)\n",
    "\n",
    "    return avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _search_image_information(objects, image_name) -> np.ndarray:\n",
    "    list_boxes = []\n",
    "    for counter,o in enumerate(objects):\n",
    "        if o['name'] != image_name:\n",
    "            continue\n",
    "\n",
    "        for b in _get_boxes(o[\"labels\"]):\n",
    "\n",
    "            current_object_label = b['category']\n",
    "\n",
    "            x1 = b['box2d']['x1']\n",
    "            y1 = b['box2d']['y1']\n",
    "            x2 = b['box2d']['x2']\n",
    "            y2 = b['box2d']['y2']\n",
    "\n",
    "            if abs(x1 - x2) < limit_box_range and abs(y1 - y2) < limit_box_range:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Last value is to determine if this box has been found or not\n",
    "            list_boxes.append([current_object_label,x1,y1,x2,y2, False])\n",
    "\n",
    "        break\n",
    "\n",
    "    return list_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_labels_ground_truth(gt_list_boxes, total_GT_correct_predicted):\n",
    "    list_ground_truth = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for counter, box in enumerate(gt_list_boxes):\n",
    "        id = _get_id_label(box[0])\n",
    "        list_ground_truth[id] = list_ground_truth[id] + 1\n",
    "        total_GT_correct_predicted[id] = total_GT_correct_predicted[id] + 1\n",
    "    return list_ground_truth,total_GT_correct_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _averageOfList(num) -> float:\n",
    "    sumOfNumbers = 0\n",
    "    for t in num:\n",
    "        sumOfNumbers = sumOfNumbers + t\n",
    "\n",
    "    avg = sumOfNumbers / len(num)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONファイルと推論結果を比較する\n",
    "def _compare_json(gt_list_boxes, objects, total_GT_correct_predicted,avg_acc_per_traffic_sign, avg_acc_per_traffic_light, \n",
    "                                avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, \n",
    "                                avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train) -> None:\n",
    "\n",
    "    for line in resultFile:\n",
    "\n",
    "        listWords = line.split(\" \")\n",
    "        if (listWords[0] == \"Enter\"):\n",
    "            # CHECK ACCURACY OF PREVIOUS IMAGE\n",
    "            if len(gt_list_boxes) > 0:\n",
    "                total_labels = len(gt_list_boxes)\n",
    "\n",
    "                list_ground_truth,total_GT_correct_predicted = _load_labels_ground_truth(gt_list_boxes,total_GT_correct_predicted)\n",
    "\n",
    "                labels_found = 0\n",
    "                # WRITE RESULTS IN A FILE\n",
    "                for box in gt_list_boxes:\n",
    "                    if box[5]:\n",
    "                        labels_found = labels_found + 1\n",
    "\n",
    "                labels_found = 0\n",
    "                for counter, items in enumerate(list_ground_truth):\n",
    "                    labels_found = labels_found + correct_predicted[counter]\n",
    "\n",
    "                f_each.append(str(g_image_name)+\" ACCURACY of labels found: \"+str(labels_found/total_labels)+\"\\n\")\n",
    "\n",
    "                avg_acc = 0\n",
    "\n",
    "                for counter, items in enumerate(list_ground_truth):\n",
    "\n",
    "                    if items == 0:\n",
    "                        f_each.append(_get_label_from_id(counter)+ \"- Correct Predicted/labels in image:\"+ str(correct_predicted[counter]) + \"/\" + str(items) + \" = 0\\n\")\n",
    "                    else:\n",
    "                        f_each.append(_get_label_from_id(counter)+ \"- Correct Predicted/labels in image:\"+ str(correct_predicted[counter]) + \"/\" + str(items) + \" = \" +  str(correct_predicted[counter]/items) + \"\\n\")\n",
    "\n",
    "                    if predicted_per_label[counter] == 0:\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- Good predicted(TP)/total predicted(FP):\"+ str(correct_predicted[counter])+ \"/\"+str(predicted_per_label[counter])+\" = 0\\n\")\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- False predicted(FP)/total predicted(FP):\"+ str(predicted_per_label[counter] - correct_predicted[counter])+ \"/\"+str(predicted_per_label[counter])+\" = 0\\n\")\n",
    "                    else:\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- Good predicted(TP)/total predicted:\"+ str(correct_predicted[counter]) + \"/\"+str(predicted_per_label[counter])+\" = \"+ str(correct_predicted[counter]/predicted_per_label[counter]) + \"\\n\")\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- False predicted(FP)/total predicted:\"+ str(predicted_per_label[counter] - correct_predicted[counter]) + \"/\"+str(predicted_per_label[counter])+\" = \"+ str((predicted_per_label[counter] - correct_predicted[counter])/predicted_per_label[counter]) + \"\\n\")\n",
    "\n",
    "\n",
    "                    if predicted_per_label[counter] > 0 and items == 0:\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- Accuracy: 0 \\n\")\n",
    "                        label_avg_acc = 0\n",
    "                        avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train = _adding_avg_label_from_id(counter, 0, avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train)\n",
    "                    elif predicted_per_label[counter] == 0 and items > 0:\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- Accuracy: 0 \\n\")\n",
    "                        label_avg_acc = 0\n",
    "                        avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train = _adding_avg_label_from_id(counter, 0, avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train)\n",
    "                    elif predicted_per_label[counter] == 0 and items == 0:\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- Accuracy: 0 \\n\")\n",
    "                        avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train = _adding_avg_label_from_id(counter, 100, avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train)\n",
    "                        label_avg_acc = 0\n",
    "                    else: # It has predicted correctly that there are any label of the object searched\n",
    "                        f_each.append(str(_get_label_from_id(counter))+ \"- Accuracy: \" + str((correct_predicted[counter]/items)*100) + \" \\n\")\n",
    "                        label_avg_acc = (correct_predicted[counter]/items)*100\n",
    "                        avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train = _adding_avg_label_from_id(counter, ((correct_predicted[counter]/items)*100), avg_acc_per_traffic_sign, avg_acc_per_traffic_light, avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train)\n",
    "\n",
    "                    avg_acc = avg_acc + label_avg_acc\n",
    "                    f_each.append(\"---\\n\")\n",
    "\n",
    "                f_each.append(str(g_image_name)+\" AVERAGE ACCURACY: \"+str(avg_acc/10)+\"\\n\")\n",
    "                average_accuracy.append(avg_acc/10)\n",
    "\n",
    "                f_each.append(\"--------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "            correct_predicted = [0,0,0,0,0,0,0,0,0,0]\n",
    "            predicted_per_label = [0,0,0,0,0,0,0,0,0,0]\n",
    "            g_image_name = listWords[3]\n",
    "\n",
    "            # ADD HERE THE INFORMATION LABELS OF THAT images\n",
    "            gt_list_boxes = _search_image_information(objects, g_image_name)\n",
    "        else:\n",
    "            # using remove() to\n",
    "            # perform removal\n",
    "            while(\"\" in listWords) :\n",
    "                listWords.remove(\"\")\n",
    "            current_object_label = listWords[0][:-1]\n",
    "\n",
    "            if current_object_label == \"traffi\":\n",
    "                current_object_label = \"traffic\" + \" \" + listWords[1][:-1]\n",
    "                count_traffic_label = 1\n",
    "            else:\n",
    "                count_traffic_label = 0\n",
    "\n",
    "\n",
    "            # 他の物体を認識したときはスルーする\n",
    "            if _get_id_label(current_object_label) == -1:\n",
    "                print(current_object_label)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Left_x\n",
    "            left_x = listWords[2+count_traffic_label]\n",
    "\n",
    "            # top_y\n",
    "            top_y = listWords[4+count_traffic_label]\n",
    "\n",
    "            # width\n",
    "            width = listWords[6+count_traffic_label]\n",
    "\n",
    "            # height\n",
    "            height = listWords[8+count_traffic_label][:-2]\n",
    "\n",
    "            # Get the X-Y info\n",
    "            x1 = int(float(left_x))\n",
    "            y1 = int(float(top_y))\n",
    "            x2 = x1 + int(float(width))\n",
    "            y2 = y1 + int(float(height))\n",
    "\n",
    "            if abs(x1 - x2) < limit_box_range and abs(y1 - y2) < limit_box_range:\n",
    "                continue\n",
    "            total_predicted_per_label[_get_id_label(current_object_label)] = total_predicted_per_label[_get_id_label(current_object_label)] + 1\n",
    "            predicted_per_label[_get_id_label(current_object_label)] = predicted_per_label[_get_id_label(current_object_label)] + 1\n",
    "\n",
    "            position = -1\n",
    "            range = 200\n",
    "\n",
    "            for counter, box in enumerate(gt_list_boxes):\n",
    "                box_label= box[0]\n",
    "                box_x1 = box[1]\n",
    "                if box_label == current_object_label:\n",
    "                    if range >= abs(box_x1 - x1):\n",
    "                        range = abs(box_x1 - x1)\n",
    "                        position = counter\n",
    "\n",
    "            if position >= 0:\n",
    "                if gt_list_boxes[position][5] == False:\n",
    "\n",
    "                    gt_list_boxes[position][5] = True\n",
    "\n",
    "                    id = _get_id_label(current_object_label)\n",
    "                    correct_predicted[id] = correct_predicted[id] + 1\n",
    "                    total_correct_predicted[id] = total_correct_predicted[id] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _accuracy_count(total_GT_correct_predicted, avg_acc_per_traffic_sign, avg_acc_per_traffic_light, \n",
    "                                avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, \n",
    "                                avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train) -> None:\n",
    "\n",
    "    f_each.append(\"\\n\")\n",
    "    f_each.append(\"TOTAL\\n\")\n",
    "    f_each.append(\"--------------------------------------------------------------------\\n\")\n",
    "    f_each.append(\"Average accuracy: \" + str(_averageOfList(average_accuracy))+\"\\n\")\n",
    "\n",
    "    string_result_amount_labels = \"\"\n",
    "    string_result_acc = \"\"\n",
    "\n",
    "    for counter,amount_label in enumerate(total_GT_correct_predicted):\n",
    "\n",
    "        f_each.append(str(_get_label_from_id(counter)) +\"- Amount of labels: \" + str(amount_label) + \"\\n\")\n",
    "\n",
    "        if counter == 0:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_traffic_sign))+\"\\n\") \n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_traffic_sign)) + \",\"\n",
    "        elif counter == 1:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_traffic_light))+\"\\n\")\n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_traffic_light)) + \",\"\n",
    "        elif counter == 2:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_car))+\"\\n\")\n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_car)) + \",\"\n",
    "        elif counter == 3:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_rider))+\"\\n\")\n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_rider)) + \",\"\n",
    "        elif counter == 4:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_motor))+\"\\n\") \n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_motor)) + \",\"\n",
    "        elif counter == 5:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_person))+\"\\n\")\n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_person)) + \",\"\n",
    "        elif counter == 6:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_bus))+\"\\n\")\n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_bus)) + \",\"\n",
    "        elif counter == 7:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_truck))+\"\\n\")\n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_truck)) + \",\"\n",
    "        elif counter == 8:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_bike))+\"\\n\") \n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_bike)) + \",\"\n",
    "        elif counter == 9:\n",
    "            f_each.append(str(_get_label_from_id(counter)) +\"- Average accuracy: \" + str(_averageOfList(avg_acc_per_train))+\"\\n\")\n",
    "            string_result_acc = string_result_acc +  str(_averageOfList(avg_acc_per_train)) + \",\"\n",
    "\n",
    "        string_result_amount_labels = string_result_amount_labels + str(amount_label) + \",\"\n",
    "\n",
    "\n",
    "        f_each.append(\"---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _view_accuracy_label(total_GT_correct_predicted) -> None:\n",
    "\n",
    "    false_result = []\n",
    "\n",
    "    for index,amount_label in enumerate(total_predicted_per_label):\n",
    "        correct_buffer = total_correct_predicted[index]\n",
    "        false_buffer = 0\n",
    "\n",
    "        if amount_label > total_GT_correct_predicted[index]: \n",
    "            false_buffer = amount_label - correct_buffer\n",
    "        else:\n",
    "            false_buffer = total_GT_correct_predicted[index] - correct_buffer\n",
    "        false_result.append(false_buffer)\n",
    "\n",
    "        if correct_buffer == 0:\n",
    "            g_total_accuracy.append(0)\n",
    "        else:\n",
    "            g_total_accuracy.append( correct_buffer / (correct_buffer + false_buffer))\n",
    "\n",
    "    df = pd.DataFrame({'Name': [\"traffic sign\",\"traffic light\",\"car\",\"rider\",\"motor\",\"person\",\"bus\",\"truck\",\"bike\",\"train\"],\n",
    "                    'Original_data': total_GT_correct_predicted, \n",
    "                    'Predicted_num': total_predicted_per_label,\n",
    "                    'Predicted_correct_num': total_correct_predicted,\n",
    "                    'Predicted_false_num': false_result,\n",
    "                    'Accuracy': g_total_accuracy})\n",
    "    print(df)\n",
    "    # resourcesに追加\n",
    "    save_all_label_accuracy_csv(df)\n",
    "    save_all_label_accuracy_png(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワイルドカードを使ってファイルの削除\n",
    "def _remove_glob(pathname, recursive=True):\n",
    "    for p in glob.glob(pathname, recursive=recursive):\n",
    "        if os.path.isfile(p):\n",
    "            os.remove(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像データの解凍\n",
    "def _load_image(file_path: str) -> np.ndarray:\n",
    "    with zipfile.ZipFile(file_path) as existing_zip:            \n",
    "\n",
    "        existing_zip.extractall('/tmp')\n",
    "\n",
    "        path_to_test_images_dir = pathlib.Path('/tmp')\n",
    "        test_image_path = sorted(list(path_to_test_images_dir.glob(\"*.jpg\")))\n",
    "        data = test_image_path\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'each_picture', '{}predict.jpg')\n",
    "def save_images(model, image_path_list, category_index, file_path: str=None) -> None:\n",
    "    out_files = []\n",
    "    \n",
    "    for image_path in image_path_list:\n",
    "        \n",
    "        # the array based representation of the image will be used later in order to prepare the\n",
    "        # result image with boxes and labels on it.\n",
    "        image_np = np.array(Image.open(image_path))\n",
    "        # Actual detection.\n",
    "        output_dict = _run_inference_for_single_image(model, image_np)\n",
    "\n",
    "        tmp_image_name = 'Enter Image Path: ' + image_path.name\n",
    "        resultFile.append(tmp_image_name)\n",
    "\n",
    "        for index, item in enumerate(output_dict['detection_boxes']):\n",
    "            kind = output_dict['detection_classes'][index]\n",
    "            accuracy = output_dict['detection_scores'][index]\n",
    "            accuracy_int = int(accuracy * 100)\n",
    "\n",
    "            # boxの位置情報はymin、xmin、ymax、xmaxで、０～１の範囲\n",
    "            # imageは1280*720なので、x軸×1280、y軸×720\n",
    "            if accuracy_int >= 50:\n",
    "                left_x = int(output_dict['detection_boxes'][index][1] * 1280)\n",
    "                top_y = int(output_dict['detection_boxes'][index][0] * 780)\n",
    "                width = int(output_dict['detection_boxes'][index][3] * 1280)\n",
    "                height = int(output_dict['detection_boxes'][index][2] * 780)\n",
    "\n",
    "                kind_name = \"\"\n",
    "                if category_index[kind]['name'] == \"bicycle\":\n",
    "                            kind_name = \"bike\"\n",
    "                elif category_index[kind]['name'] == \"motorcycle\":\n",
    "                            kind_name = \"motor\"\n",
    "                else:\n",
    "                            kind_name = category_index[kind]['name']\n",
    "\n",
    "                tmp_buffer = kind_name + ': ' + str(accuracy_int) + '%\\t(left_x: ' + str(left_x) + \\\n",
    "                                        '\t top_y: ' + str(top_y) + '\t width: ' + str(width) + '\t height: ' + str(height) + ')'\n",
    "                resultFile.append(tmp_buffer)\n",
    "\n",
    "\n",
    "        # Visualization of the results of a detection.\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                output_dict['detection_boxes'],\n",
    "                output_dict['detection_classes'],\n",
    "                output_dict['detection_scores'],\n",
    "                category_index,\n",
    "                instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "                use_normalized_coordinates=True,\n",
    "                min_score_thresh=.5,\n",
    "                line_thickness=8)\n",
    "\n",
    "        predict_image = file_path.format(image_path.name[0:-4])\n",
    "        out_files.append(predict_image)\n",
    "        Image.fromarray(image_np).save(predict_image)\n",
    "\n",
    "    return out_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@resources(ait_output, path_helper, 'all_label_accuracy_csv', 'all_label_accuracy.csv')\n",
    "def save_all_label_accuracy_csv(df, file_path: str=None) -> None:\n",
    "    df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@resources(ait_output, path_helper, 'all_label_accuracy_png', 'all_label_accuracy.png')\n",
    "def save_all_label_accuracy_png(df, file_path: str=None) -> None:\n",
    "    # 以前の画像ファイルがあれば削除\n",
    "    _remove_glob(str(Path(file_path).parent)+ '/*.png')\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8), dpi=100)\n",
    "    ax = fig.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    tmp_table = ax.table(cellText=df.values,\n",
    "        colLabels=df.columns,\n",
    "        rowLabels=df.index,\n",
    "        bbox=[0,0,1,1])\n",
    "    plt.savefig(file_path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'each_label_accuracy', 'each_label_accuracy.csv')\n",
    "def save_each_label_accuracy(file_path: str=None) -> None:\n",
    "    with open(file_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for buf in f_each:\n",
    "            writer.writerow(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'traffic_sign_accuracy')\n",
    "def measure_traffic_sign_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'traffic_light_accuracy')\n",
    "def measure_traffic_light_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'car_accuracy')\n",
    "def measure_car_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'rider_accuracy')\n",
    "def measure_rider_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'motor_accuracy')\n",
    "def measure_motor_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'person_accuracy')\n",
    "def measure_person_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'bus_accuracy')\n",
    "def measure_bus_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'truck_accuracy')\n",
    "def measure_truck_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'bike_accuracy')\n",
    "def measure_bike_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'train_accuracy')\n",
    "def measure_train_accuracy(accuracy):\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'Log', 'ait.log')\n",
    "def move_log(file_path: str=None) -> None:\n",
    "    shutil.move(get_log_path(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:main\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@ait_main(ait_output, path_helper)\n",
    "def main() -> None:\n",
    "\n",
    "    # インベントリのMNISTラベル・画像を読み込み\n",
    "    image_path = ait_input.get_inventory_path('test_set_images')\n",
    "    \n",
    "    model_checkpoint_path = ait_input.get_inventory_path('trained_model_checkpoint')\n",
    "    model_graph_path = ait_input.get_inventory_path('trained_model_graph')\n",
    "    model_protobuf_path = ait_input.get_inventory_path('trained_model_protobuf')\n",
    "\n",
    "    # モデル読み込み\n",
    "    zipfile.ZipFile(model_checkpoint_path).extractall('/tmp')\n",
    "    zipfile.ZipFile(model_graph_path).extractall('/tmp/ssd_mobilenet_v2_coco')\n",
    "    zipfile.ZipFile(model_protobuf_path).extractall('/tmp/ssd_mobilenet_v2_coco')\n",
    "    model = tf.saved_model.load(str('/tmp/ssd_mobilenet_v2_coco/saved_model'))\n",
    "    \n",
    "    # jsonファイルのパス\n",
    "    jsonFileName = ait_input.get_inventory_path('test_set_labels')\n",
    "    objects = _read_labels(jsonFileName)\n",
    "\n",
    "    # ラベルの定義読み込み\n",
    "    label_map_name = ait_input.get_inventory_path('labels_define')\n",
    "    category_index = label_map_util.create_category_index_from_labelmap(label_map_name, use_display_name=True)\n",
    "    \n",
    "    # 画像読み込み\n",
    "    image_list = _load_image(image_path)\n",
    "    \n",
    "\n",
    "    # 推論 (resources)\n",
    "    save_images(model, image_list, category_index)\n",
    "\n",
    "    resultFile.append('Enter Image Path: END')\n",
    "\n",
    "    # JSONファイルと推論結果を比較する\n",
    "    _compare_json(gt_list_boxes, objects, total_GT_correct_predicted, avg_acc_per_traffic_sign, avg_acc_per_traffic_light, \n",
    "                                avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, \n",
    "                                avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train)\n",
    "    # 推論結果の集計\n",
    "    _accuracy_count(total_GT_correct_predicted, avg_acc_per_traffic_sign, avg_acc_per_traffic_light, \n",
    "                                avg_acc_per_car, avg_acc_per_rider, avg_acc_per_motor, avg_acc_per_person, \n",
    "                                avg_acc_per_bus, avg_acc_per_truck, avg_acc_per_bike, avg_acc_per_train)\n",
    "    # label毎に確率を集計 (resources)\n",
    "    _view_accuracy_label(total_GT_correct_predicted)\n",
    "    \n",
    "    # download\n",
    "    save_each_label_accuracy()\n",
    "\n",
    "    # measure\n",
    "    measure_traffic_sign_accuracy(g_total_accuracy[0])\n",
    "    measure_traffic_light_accuracy(g_total_accuracy[1])\n",
    "    measure_car_accuracy(g_total_accuracy[2])\n",
    "    measure_rider_accuracy(g_total_accuracy[3])\n",
    "    measure_motor_accuracy(g_total_accuracy[4])\n",
    "    measure_person_accuracy(g_total_accuracy[5])\n",
    "    measure_bus_accuracy(g_total_accuracy[6])\n",
    "    measure_truck_accuracy(g_total_accuracy[7])\n",
    "    measure_bike_accuracy(g_total_accuracy[8])\n",
    "    measure_train_accuracy(g_total_accuracy[9])\n",
    "\n",
    "    \n",
    "    move_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Name  Original_data  Predicted_num  Predicted_correct_num  \\\n",
      "0   traffic sign             40              0                      0   \n",
      "1  traffic light             51              3                      3   \n",
      "2            car             88             35                     32   \n",
      "3          rider              8              0                      0   \n",
      "4          motor              5              1                      1   \n",
      "5         person             39             14                     11   \n",
      "6            bus              5              4                      2   \n",
      "7          truck              2              2                      1   \n",
      "8           bike              6              1                      1   \n",
      "9          train              0              0                      0   \n",
      "\n",
      "   Predicted_false_num  Accuracy  \n",
      "0                   40  0.000000  \n",
      "1                   48  0.058824  \n",
      "2                   56  0.363636  \n",
      "3                    8  0.000000  \n",
      "4                    4  0.200000  \n",
      "5                   28  0.282051  \n",
      "6                    3  0.400000  \n",
      "7                    1  0.500000  \n",
      "8                    5  0.166667  \n",
      "9                    0  0.000000  \n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# area:entory point\n",
    "# do not edit\n",
    "#########################################\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:license attribute set\n",
    "# should edit\n",
    "#########################################\n",
    "ait_owner='AIST'\n",
    "ait_creation_year='2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:prepare deproy\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.deploy import prepare_deploy\n",
    "    from ait_sdk.license.license_generator import LicenseGenerator\n",
    "    \n",
    "    current_dir = %pwd\n",
    "    prepare_deploy(ait_manifest, ait_sdk_name, current_dir, requirements_path, is_remote_deploy=True)\n",
    "    \n",
    "    # output License.txt\n",
    "    license_generator = LicenseGenerator()\n",
    "    license_generator.write('../top_dir/LICENSE.txt', ait_creation_year, ait_owner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
