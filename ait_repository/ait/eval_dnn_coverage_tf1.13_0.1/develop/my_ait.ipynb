{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# AIT Development notebook\n",
    "\n",
    "\n",
    "## notebook of structure\n",
    "\n",
    "|#|area name|cell num|description|edit or not|\n",
    "|---|---|---|---|---|\n",
    "| 1|flags set|1|setting of launch jupyter or ait flag.|no edit|\n",
    "| 2|ait-sdk install|1|Use only jupyter launch.<br>find ait-sdk and install.|no edit|\n",
    "| 3|create requirements and pip install|3|Use only jupyter launch.<br>create requirements.txt.<br>And install by requirements.txt.|should edit(second cell, you set use modules.)|\n",
    "| 4|import|2|you should write use import modules.<br>but bottom lines do not edit.|should edit(first cell, you import your moduel.)|\n",
    "| 5|create manifest|1|Use only jupyter launch.<br>create ait.manifest.json.|should edit|\n",
    "| 6|create input|1|Use only jupyter launch.<br>create ait.input.json.|should edit|\n",
    "| 7|initialize|1|this cell is initialize for ait progress.|no edit|\n",
    "| 8|functions|N|you defined measures, resources, downloads in ait.manifesit.json. <br>Define any functions to add these.|should edit|\n",
    "| 9|main|1|Read the data set or model and calls the function defined in `functions-area`.|should edit|\n",
    "|10|entrypoint|1|Call the main function.|no edit|\n",
    "|11|license attribute set|1|Use only notebook launch.<br>Setting attribute for license.|should edit|\n",
    "|12|prepare deploy|1|Use only notebook launch.<br>Convert to python programs and create dag.py.|no edit|\n",
    "\n",
    "## notebook template revision history\n",
    "\n",
    "### 1.0.1 2020/10/21\n",
    "\n",
    "* add revision history\n",
    "* separate `create requirements and pip install` editable and noeditable\n",
    "* separate `import` editable and noeditable\n",
    "\n",
    "### 1.0.0 2020/10/12\n",
    "\n",
    "* new cerarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:flags set\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "# Determine whether to start AIT or jupyter by startup argument\n",
    "import sys\n",
    "is_ait_launch = (len(sys.argv) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.2.4)\n",
      "Processing ./ait_sdk-0.1.4-py3-none-any.whl\n",
      "Collecting nbconvert\n",
      "  Using cached nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting nbformat\n",
      "  Using cached nbformat-5.0.8-py3-none-any.whl (172 kB)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 5.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 4.9 MB/s eta 0:00:01    |████████████▌                   | 5.7 MB 7.1 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting psutil\n",
      "  Downloading psutil-5.7.3.tar.gz (465 kB)\n",
      "\u001b[K     |████████████████████████████████| 465 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting testpath\n",
      "  Using cached testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
      "Collecting pygments>=2.4.1\n",
      "  Downloading Pygments-2.7.2-py3-none-any.whl (948 kB)\n",
      "\u001b[K     |████████████████████████████████| 948 kB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting defusedxml\n",
      "  Using cached defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting jinja2>=2.4\n",
      "  Using cached Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Using cached traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-3.2.1-py2.py3-none-any.whl (145 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting entrypoints>=0.2.2\n",
      "  Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Using cached nbclient-0.5.1-py3-none-any.whl (65 kB)\n",
      "Collecting jupyter-core\n",
      "  Using cached jupyter_core-4.6.3-py2.py3-none-any.whl (83 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.4.3.tar.gz (16 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.14\n",
      "  Downloading scipy-1.5.3-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 17.2 MB/s eta 0:00:01    |▋                               | 491 kB 13.6 MB/s eta 0:00:02     |█████████▌                      | 7.7 MB 13.6 MB/s eta 0:00:02     |███████████████▍                | 12.4 MB 7.0 MB/s eta 0:00:02     |████████████████▋               | 13.4 MB 7.0 MB/s eta 0:00:02     |███████████████████▌            | 15.7 MB 7.0 MB/s eta 0:00:02     |█████████████████████████▏      | 20.3 MB 7.0 MB/s eta 0:00:01     |██████████████████████████▋     | 21.5 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema!=2.5.0,>=2.4\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting ipython-genutils\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Using cached MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting decorator\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jupyter-client>=6.1.5\n",
      "  Using cached jupyter_client-6.1.7-py3-none-any.whl (108 kB)\n",
      "Collecting async-generator\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting nest-asyncio\n",
      "  Downloading nest_asyncio-1.4.2-py3-none-any.whl (5.3 kB)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-50.3.2-py3-none-any.whl (785 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Using cached attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
      "Processing /root/.cache/pip/wheels/34/13/19/294da8e11bce7e563afee51251b9fa878185e14f4b5caf00cb/pyrsistent-0.17.3-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-2.0.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting pyzmq>=13\n",
      "  Using cached pyzmq-19.0.2-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Processing /root/.cache/pip/wheels/37/a7/db/2d592e44029ef817f3ef63ea991db34191cebaef087a96f505/tornado-6.0.4-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Building wheels for collected packages: py-cpuinfo, psutil, pandocfilters, pyyaml\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-py3-none-any.whl size=20299 sha256=c74e5843c010a56ed412b3405dc55b3043a3eea508648895bbb7d28737de9127\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/6d/cc/73a126dc2e09fe56fcec0a7386d255762611fbed1c86d3bbcc\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.3-cp36-cp36m-linux_x86_64.whl size=288565 sha256=33e2a07bad0a4bdf53c841b18fed75e9d922a141bcf872afc02e79cfe27ba6f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/ad/67/90bbaacdcfe970960dd5158397f23a6579b51d853720d7856d\n",
      "  Building wheel for pandocfilters (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=10627 sha256=68bb1e4a0a8a223fd15889f65c4eb40b5f183767a38dad99cced55df00db8444\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/12/89/fe63ac4d6ee6440daab4db77b78c63f7f192b700f844b6639f\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=45919 sha256=71428dab02c2e0ba92da35c716d4c1c531b27c4acaa2eb4f0cddfe85ed29bd03\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "Successfully built py-cpuinfo psutil pandocfilters pyyaml\n",
      "Installing collected packages: pygments, jupyterlab-pygments, testpath, defusedxml, MarkupSafe, jinja2, decorator, six, ipython-genutils, traitlets, pyparsing, packaging, webencodings, bleach, setuptools, attrs, pyrsistent, zipp, importlib-metadata, jsonschema, jupyter-core, nbformat, mistune, entrypoints, pyzmq, python-dateutil, tornado, jupyter-client, async-generator, nest-asyncio, nbclient, pandocfilters, nbconvert, pyyaml, numpy, scipy, h5py, keras, py-cpuinfo, psutil, ait-sdk\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.7.1\n",
      "    Uninstalling Pygments-2.7.1:\n",
      "      Successfully uninstalled Pygments-2.7.1\n",
      "  Attempting uninstall: jupyterlab-pygments\n",
      "    Found existing installation: jupyterlab-pygments 0.1.2\n",
      "    Uninstalling jupyterlab-pygments-0.1.2:\n",
      "      Successfully uninstalled jupyterlab-pygments-0.1.2\n",
      "  Attempting uninstall: testpath\n",
      "    Found existing installation: testpath 0.4.4\n",
      "    Uninstalling testpath-0.4.4:\n",
      "      Successfully uninstalled testpath-0.4.4\n",
      "  Attempting uninstall: defusedxml\n",
      "    Found existing installation: defusedxml 0.6.0\n",
      "    Uninstalling defusedxml-0.6.0:\n",
      "      Successfully uninstalled defusedxml-0.6.0\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.2\n",
      "    Uninstalling Jinja2-2.11.2:\n",
      "      Successfully uninstalled Jinja2-2.11.2\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 4.4.2\n",
      "    Uninstalling decorator-4.4.2:\n",
      "      Successfully uninstalled decorator-4.4.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: ipython-genutils\n",
      "    Found existing installation: ipython-genutils 0.2.0\n",
      "    Uninstalling ipython-genutils-0.2.0:\n",
      "      Successfully uninstalled ipython-genutils-0.2.0\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 4.3.3\n",
      "    Uninstalling traitlets-4.3.3:\n",
      "      Successfully uninstalled traitlets-4.3.3\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "  Attempting uninstall: webencodings\n",
      "    Found existing installation: webencodings 0.5.1\n",
      "    Uninstalling webencodings-0.5.1:\n",
      "      Successfully uninstalled webencodings-0.5.1\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 3.2.1\n",
      "    Uninstalling bleach-3.2.1:\n",
      "      Successfully uninstalled bleach-3.2.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 50.3.2\n",
      "    Uninstalling setuptools-50.3.2:\n",
      "      Successfully uninstalled setuptools-50.3.2\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.2.0\n",
      "    Uninstalling attrs-20.2.0:\n",
      "      Successfully uninstalled attrs-20.2.0\n",
      "  Attempting uninstall: pyrsistent\n",
      "    Found existing installation: pyrsistent 0.17.3\n",
      "    Uninstalling pyrsistent-0.17.3:\n",
      "      Successfully uninstalled pyrsistent-0.17.3\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.1.0\n",
      "    Uninstalling zipp-3.1.0:\n",
      "      Successfully uninstalled zipp-3.1.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 1.7.0\n",
      "    Uninstalling importlib-metadata-1.7.0:\n",
      "      Successfully uninstalled importlib-metadata-1.7.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 3.2.0\n",
      "    Uninstalling jsonschema-3.2.0:\n",
      "      Successfully uninstalled jsonschema-3.2.0\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.6.3\n",
      "    Uninstalling jupyter-core-4.6.3:\n",
      "      Successfully uninstalled jupyter-core-4.6.3\n",
      "  Attempting uninstall: nbformat\n",
      "    Found existing installation: nbformat 5.0.8\n",
      "    Uninstalling nbformat-5.0.8:\n",
      "      Successfully uninstalled nbformat-5.0.8\n",
      "  Attempting uninstall: mistune\n",
      "    Found existing installation: mistune 0.8.4\n",
      "    Uninstalling mistune-0.8.4:\n",
      "      Successfully uninstalled mistune-0.8.4\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.3\n",
      "    Uninstalling entrypoints-0.3:\n",
      "      Successfully uninstalled entrypoints-0.3\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 19.0.2\n",
      "    Uninstalling pyzmq-19.0.2:\n",
      "      Successfully uninstalled pyzmq-19.0.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.0.4\n",
      "    Uninstalling tornado-6.0.4:\n",
      "      Successfully uninstalled tornado-6.0.4\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 6.1.7\n",
      "    Uninstalling jupyter-client-6.1.7:\n",
      "      Successfully uninstalled jupyter-client-6.1.7\n",
      "  Attempting uninstall: async-generator\n",
      "    Found existing installation: async-generator 1.10\n",
      "    Uninstalling async-generator-1.10:\n",
      "      Successfully uninstalled async-generator-1.10\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.4.1\n",
      "    Uninstalling nest-asyncio-1.4.1:\n",
      "      Successfully uninstalled nest-asyncio-1.4.1\n",
      "  Attempting uninstall: nbclient\n",
      "    Found existing installation: nbclient 0.5.1\n",
      "    Uninstalling nbclient-0.5.1:\n",
      "      Successfully uninstalled nbclient-0.5.1\n",
      "  Attempting uninstall: pandocfilters\n",
      "    Found existing installation: pandocfilters 1.4.2\n",
      "    Uninstalling pandocfilters-1.4.2:\n",
      "      Successfully uninstalled pandocfilters-1.4.2\n",
      "  Attempting uninstall: nbconvert\n",
      "    Found existing installation: nbconvert 6.0.7\n",
      "    Uninstalling nbconvert-6.0.7:\n",
      "      Successfully uninstalled nbconvert-6.0.7\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.2 which is incompatible.\n",
      "tensorflow 2.3.0 requires scipy==1.4.1, but you'll have scipy 1.5.3 which is incompatible.\u001b[0m\n",
      "Successfully installed MarkupSafe-1.1.1 ait-sdk-0.1.4 async-generator-1.10 attrs-20.2.0 bleach-3.2.1 decorator-4.4.2 defusedxml-0.6.0 entrypoints-0.3 h5py-2.10.0 importlib-metadata-2.0.0 ipython-genutils-0.2.0 jinja2-2.11.2 jsonschema-3.2.0 jupyter-client-6.1.7 jupyter-core-4.6.3 jupyterlab-pygments-0.1.2 keras-2.4.3 mistune-0.8.4 nbclient-0.5.1 nbconvert-6.0.7 nbformat-5.0.8 nest-asyncio-1.4.2 numpy-1.19.2 packaging-20.4 pandocfilters-1.4.3 psutil-5.7.3 py-cpuinfo-7.0.0 pygments-2.7.2 pyparsing-2.4.7 pyrsistent-0.17.3 python-dateutil-2.8.1 pyyaml-5.3.1 pyzmq-19.0.2 scipy-1.5.3 setuptools-50.3.2 six-1.15.0 testpath-0.4.4 tornado-6.0.4 traitlets-4.3.3 webencodings-0.5.1 zipp-3.4.0\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# area:ait-sdk install\n",
    "# do not edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    # get ait-sdk file name\n",
    "    from pathlib import Path\n",
    "    from glob import glob\n",
    "    import re\n",
    "\n",
    "    def numericalSort(value):\n",
    "        numbers = re.compile(r'(\\d+)')\n",
    "        parts = numbers.split(value)\n",
    "        parts[1::2] = map(int, parts[1::2])\n",
    "        return parts\n",
    "    latest_sdk_file_path=sorted(glob('../lib/*.whl'), key=numericalSort)[-1]\n",
    "\n",
    "    ait_sdk_name = Path(latest_sdk_file_path).name\n",
    "    \n",
    "    # copy to develop dir\n",
    "    import shutil\n",
    "    current_dir = %pwd\n",
    "    shutil.copyfile(f'../lib/{ait_sdk_name}', f'{current_dir}/{ait_sdk_name}')\n",
    "\n",
    "    # install ait-sdk\n",
    "    !pip install --upgrade pip\n",
    "    !pip install --force-reinstall ./$ait_sdk_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create requirements and pip install\n",
    "# do not edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_requirements_generator import AITRequirementsGenerator\n",
    "    requirements_generator = AITRequirementsGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create requirements and pip install\n",
    "# should edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    requirements_generator.add_package('Cython', '0.29.21')\n",
    "    requirements_generator.add_package('bleach','1.5.0')\n",
    "    requirements_generator.add_package('cycler','0.10.0')\n",
    "    requirements_generator.add_package('decorator','4.1.2')\n",
    "    requirements_generator.add_package('h5py','2.9.0')\n",
    "    requirements_generator.add_package('html5lib','0.9999999')\n",
    "    requirements_generator.add_package('Keras','2.0.8')\n",
    "    requirements_generator.add_package('Markdown','2.6.9')\n",
    "    requirements_generator.add_package('matplotlib','2.0.2')\n",
    "    requirements_generator.add_package('networkx','1.11')\n",
    "    requirements_generator.add_package('numpy','1.19.2')\n",
    "    requirements_generator.add_package('olefile','0.44')\n",
    "    requirements_generator.add_package('pandas','0.20.3')\n",
    "    requirements_generator.add_package('Pillow','4.3.0')\n",
    "    requirements_generator.add_package('protobuf','3.6.1')\n",
    "    requirements_generator.add_package('pyparsing','2.2.0')\n",
    "    requirements_generator.add_package('python-dateutil','2.6.1')\n",
    "    requirements_generator.add_package('pytz','2017.2')\n",
    "    requirements_generator.add_package('PyWavelets','1.1.1')\n",
    "    requirements_generator.add_package('PyYAML','3.12')\n",
    "    requirements_generator.add_package('scikit-image','0.14.2')\n",
    "    requirements_generator.add_package('scikit-learn','0.19.0')\n",
    "    requirements_generator.add_package('scipy','0.19.1')\n",
    "    requirements_generator.add_package('six','1.11.0')\n",
    "    requirements_generator.add_package('tensorflow','1.13.1')\n",
    "    requirements_generator.add_package('tensorflow-tensorboard','0.1.6')\n",
    "    requirements_generator.add_package('Werkzeug','0.12.2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r /workdir/root/develop/requirements.txt (line 3)) (0.10.0)\n",
      "Collecting decorator==4.1.2\n",
      "  Downloading decorator-4.1.2-py2.py3-none-any.whl (9.1 kB)\n",
      "Collecting h5py==2.9.0\n",
      "  Downloading h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999\n",
      "  Downloading html5lib-0.9999999.tar.gz (889 kB)\n",
      "\u001b[K     |████████████████████████████████| 889 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Keras==2.0.8\n",
      "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[K     |████████████████████████████████| 276 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Markdown==2.6.9\n",
      "  Downloading Markdown-2.6.9.tar.gz (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==2.0.2\n",
      "  Downloading matplotlib-2.0.2-cp36-cp36m-manylinux1_x86_64.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx==1.11\n",
      "  Downloading networkx-1.11-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.6/dist-packages (from -r /workdir/root/develop/requirements.txt (line 11)) (1.19.2)\n",
      "Collecting olefile==0.44\n",
      "  Downloading olefile-0.44.zip (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pandas==0.20.3\n",
      "  Downloading pandas-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 21.4 MB/s eta 0:00:01     |████████████████████████████▌   | 21.8 MB 21.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow==4.3.0\n",
      "  Downloading Pillow-4.3.0-cp36-cp36m-manylinux1_x86_64.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf==3.6.1\n",
      "  Downloading protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing==2.2.0\n",
      "  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil==2.6.1\n",
      "  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz==2017.2\n",
      "  Downloading pytz-2017.2-py2.py3-none-any.whl (484 kB)\n",
      "\u001b[K     |████████████████████████████████| 484 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets==1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML==3.12\n",
      "  Downloading PyYAML-3.12.tar.gz (253 kB)\n",
      "\u001b[K     |████████████████████████████████| 253 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image==0.14.2\n",
      "  Downloading scikit_image-0.14.2-cp36-cp36m-manylinux1_x86_64.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 10.9 MB/s eta 0:00:01    |███████████████▎                | 12.1 MB 6.7 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting scikit-learn==0.19.0\n",
      "  Downloading scikit_learn-0.19.0-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 13.1 MB/s eta 0:00:01     |█████████████████████████████   | 11.3 MB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==0.19.1\n",
      "  Downloading scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 48.2 MB 11.1 MB/s eta 0:00:01     |█████████████████████▊          | 32.7 MB 9.8 MB/s eta 0:00:02     |███████████████████████▋        | 35.6 MB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.11.0\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 92.5 MB 15.9 MB/s eta 0:00:01     |████▍                           | 12.7 MB 15.8 MB/s eta 0:00:06     |████████                        | 23.1 MB 10.5 MB/s eta 0:00:07     |███████████████████████████████▎| 90.5 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-tensorboard==0.1.6\n",
      "  Downloading tensorflow_tensorboard-0.1.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Werkzeug==0.12.2\n",
      "  Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ait-sdk==0.1.4 from file:///workdir/root/develop/ait_sdk-0.1.4-py3-none-any.whl in /usr/local/lib/python3.6/dist-packages (from -r /workdir/root/develop/requirements.txt (line 28)) (0.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf==3.6.1->-r /workdir/root/develop/requirements.txt (line 15)) (50.3.2)\n",
      "Collecting dask[array]>=1.0.0\n",
      "  Downloading dask-2.30.0-py3-none-any.whl (848 kB)\n",
      "\u001b[K     |████████████████████████████████| 848 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle>=0.2.1\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /workdir/root/develop/requirements.txt (line 25)) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /workdir/root/develop/requirements.txt (line 25)) (1.30.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /workdir/root/develop/requirements.txt (line 25)) (1.1.2)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[K     |████████████████████████████████| 367 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.13.1->-r /workdir/root/develop/requirements.txt (line 25)) (0.30.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /workdir/root/develop/requirements.txt (line 25)) (1.1.0)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /workdir/root/develop/requirements.txt (line 25)) (0.3.3)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 10.6 MB/s eta 0:00:01     |████████████████████            | 2.0 MB 10.6 MB/s eta 0:00:01     |███████████████████████████████▊| 3.2 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (5.0.8)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.6/dist-packages (from ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (7.0.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (6.0.7)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (5.7.3)\n",
      "Collecting toolz>=0.8.2; extra == \"array\"\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (4.6.3)\n",
      "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (4.3.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.5.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (2.7.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (1.4.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.6.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (2.11.2)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.4.4)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (20.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (2.0.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (1.4.2)\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (1.10)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (6.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (3.4.0)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (6.0.4)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert->ait-sdk==0.1.4->-r /workdir/root/develop/requirements.txt (line 28)) (19.0.2)\n",
      "Building wheels for collected packages: html5lib, Markdown, olefile, PyYAML\n",
      "  Building wheel for html5lib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for html5lib: filename=html5lib-0.9999999-py3-none-any.whl size=111295 sha256=628c299d2a1ebd91d58b9562dbeb9e9cfea19f777c4ab43a2dbf7b003037d3db\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/1c/cb/a87fd097ff74648ecc468a703001f6c7c86d8a71d459e65c98\n",
      "  Building wheel for Markdown (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Markdown: filename=Markdown-2.6.9-py3-none-any.whl size=163233 sha256=28888681131f22d1d802cc46fefa7fc414b569dc6e8ef77dd75ba79b4986263b\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/55/17/f129d80a5d263161d52b3c282fd818317dc95986535b7ee24a\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.44-py3-none-any.whl size=52631 sha256=98b2dbe677d92d94c4cc5f8e8e18f16d41438950a84cd037cc88272ff7033fc5\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/77/c6/3d974ba3cb5825fb376485fc5abd5c7a427b85b187a611fecc\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-3.12-cp36-cp36m-linux_x86_64.whl size=43458 sha256=48296b55f9a38941d420a771a8f949b6c6cb03eea57c54b8b8822689bc852891\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/06/c3/a48dc77c7d8f60b64eaf0fffd5ee5ab8abce5d13893b73109b\n",
      "Successfully built html5lib Markdown olefile PyYAML\n",
      "Installing collected packages: Cython, six, html5lib, bleach, decorator, h5py, scipy, PyYAML, Keras, Markdown, pyparsing, pytz, python-dateutil, matplotlib, networkx, olefile, pandas, Pillow, protobuf, PyWavelets, toolz, dask, cloudpickle, scikit-image, scikit-learn, astor, mock, tensorflow-estimator, keras-applications, Werkzeug, tensorboard, tensorflow, tensorflow-tensorboard\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 3.2.1\n",
      "    Uninstalling bleach-3.2.1:\n",
      "      Successfully uninstalled bleach-3.2.1\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 4.4.2\n",
      "    Uninstalling decorator-4.4.2:\n",
      "      Successfully uninstalled decorator-4.4.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.3\n",
      "    Uninstalling scipy-1.5.3:\n",
      "      Successfully uninstalled scipy-1.5.3\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.3.1\n",
      "    Uninstalling PyYAML-5.3.1:\n",
      "      Successfully uninstalled PyYAML-5.3.1\n",
      "  Attempting uninstall: Keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "  Attempting uninstall: Markdown\n",
      "    Found existing installation: Markdown 3.2.2\n",
      "    Uninstalling Markdown-3.2.2:\n",
      "      Successfully uninstalled Markdown-3.2.2\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.2\n",
      "    Uninstalling matplotlib-3.3.2:\n",
      "      Successfully uninstalled matplotlib-3.3.2\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.0.0\n",
      "    Uninstalling Pillow-8.0.0:\n",
      "      Successfully uninstalled Pillow-8.0.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.12.2\n",
      "    Uninstalling protobuf-3.12.2:\n",
      "      Successfully uninstalled protobuf-3.12.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.1\n",
      "    Uninstalling Werkzeug-1.0.1:\n",
      "      Successfully uninstalled Werkzeug-1.0.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed Cython-0.29.21 Keras-2.0.8 Markdown-2.6.9 Pillow-4.3.0 PyWavelets-1.1.1 PyYAML-3.12 Werkzeug-0.12.2 astor-0.8.1 bleach-1.5.0 cloudpickle-1.6.0 dask-2.30.0 decorator-4.1.2 h5py-2.9.0 html5lib-0.9999999 keras-applications-1.0.8 matplotlib-2.0.2 mock-4.0.2 networkx-1.11 olefile-0.44 pandas-0.20.3 protobuf-3.6.1 pyparsing-2.2.0 python-dateutil-2.6.1 pytz-2017.2 scikit-image-0.14.2 scikit-learn-0.19.0 scipy-0.19.1 six-1.11.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 tensorflow-tensorboard-0.1.6 toolz-0.11.1\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# area:create requirements and pip install\n",
    "# do not edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    requirements_generator.add_package(f'./{ait_sdk_name}')\n",
    "    requirements_path = requirements_generator.create_requirements(current_dir)\n",
    "\n",
    "    !pip install -r $requirements_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# area:import\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "# import if you need modules cell\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from os import makedirs, path\n",
    "from glob import glob\n",
    "import csv\n",
    "from deep_saucer.neuron_coverage.tensorflow_native.lib.coverage_verification import main_test\n",
    "from ait_sdk.utils.mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:import\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "# must use modules\n",
    "import shutil  # do not remove\n",
    "from ait_sdk.common.files.ait_input import AITInput  # do not remove\n",
    "from ait_sdk.common.files.ait_output import AITOutput  # do not remove\n",
    "from ait_sdk.common.files.ait_manifest import AITManifest  # do not remove\n",
    "from ait_sdk.develop.ait_path_helper import AITPathHelper  # do not remove\n",
    "from ait_sdk.utils.logging import get_logger, log, get_log_path  # do not remove\n",
    "from ait_sdk.develop.annotation import measures, resources, downloads, ait_main  # do not remove\n",
    "# must use modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create manifest\n",
    "# should edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_manifest_generator import AITManifestGenerator\n",
    "    \n",
    "    manifest_genenerator = AITManifestGenerator(current_dir)\n",
    "    manifest_genenerator.set_ait_name('eval_dnn_coverage_tf1.13')\n",
    "    manifest_genenerator.set_ait_description('''\n",
    "    Calculate the neuron coverage for the input dataset given by the user, and display it in the form of a heat map. After that, a gradient for the input dataset is computed by backpropagation. Based on the gradient, the most efficient manipulation to input values in order for the neuron coverage to increase is selected. A number of data is removed from the dataset, and these removed data are converted to new data by the selected manipulation. Then, the new data is added to the dataset, and the coverage is recalculated by running the model with the updated dataset. Similarly, a gradient for the updated dataset is computed. These processes are repeated until the target coverage rate is achieved. Data manipulation algorithm is implemented by the user in Python.\n",
    "    ''')\n",
    "    manifest_genenerator.set_ait_author('AIST')\n",
    "    manifest_genenerator.set_ait_email('')\n",
    "    manifest_genenerator.set_ait_version('0.1')\n",
    "    manifest_genenerator.set_ait_quality('https://airc.aist.go.jp/aiqm/quality/internal/機械学習モデルの安定性')\n",
    "    manifest_genenerator.set_ait_reference('')\n",
    "    manifest_genenerator.add_ait_inventories(name='image_data', \n",
    "                                             type_='dataset', \n",
    "                                             description='MNIST image data', \n",
    "                                             format_=['gz'], \n",
    "                                             schema='http://yann.lecun.com/exdb/mnist/')\n",
    "    manifest_genenerator.add_ait_inventories(name='label', \n",
    "                                             type_='dataset', \n",
    "                                             description='MNIST label data', \n",
    "                                             format_=['gz'], \n",
    "                                             schema='http://yann.lecun.com/exdb/mnist/')\n",
    "    manifest_genenerator.add_ait_inventories(name='tf_ckpt', \n",
    "                                             type_='model', \n",
    "                                             description='''Tensorflow model datas.\\n\n",
    "                                             This is loaded by `tf.train.import_meta_graph`.''', \n",
    "                                             format_=['*'], \n",
    "                                             schema='https://github.com/tensorflow/models/tree/master/official')\n",
    "    manifest_genenerator.add_ait_parameters(name='mnist_image_px_size', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            MNIST Imagge pixel size.\n",
    "                                            ''', \n",
    "                                            default_val='28')\n",
    "    manifest_genenerator.add_ait_parameters(name='determination_on_activation', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Neuron Activity/Inactivity\\n\n",
    "                                            Determination Type\\n\n",
    "                                            0: Threshold Determination\\n\n",
    "                                            1: Upper/Lower Limit Determination\\n\n",
    "                                            2: N Cases of Maximum Value Determination\n",
    "                                            ''', \n",
    "                                            default_val='0')\n",
    "    manifest_genenerator.add_ait_parameters(name='threshold', \n",
    "                                            type_='float', \n",
    "                                            description='''\n",
    "                                            Threshold\\n\n",
    "                                            Valid only for threshold determination\n",
    "                                            ''', \n",
    "                                            default_val='0')\n",
    "    manifest_genenerator.add_ait_parameters(name='lower_bound', \n",
    "                                            type_='float', \n",
    "                                            description='''\n",
    "                                            Lower Limit\\n\n",
    "                                            No default value\\n\n",
    "                                            (Valid only for upper/lower limit determination)\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='upper_bound', \n",
    "                                            type_='float', \n",
    "                                            description='''\n",
    "                                            Upper Limit\\n\n",
    "                                            No default value\\n\n",
    "                                            (Valid only for upper/lower limit determination\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='activation_filter_no', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Number of Cases N\\n\n",
    "                                            The default value is 1\\n\n",
    "                                            (Valid only for N Cases of maximum value determination)\n",
    "                                            ''', \n",
    "                                            default_val='1')\n",
    "    manifest_genenerator.add_ait_parameters(name='heat_map_type', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Heat Map Generation Method Type\\n\n",
    "                                            The default value is 1\\n\n",
    "                                            0: Generation Not Necessary\\n\n",
    "                                            1: 0/1 Table\\n\n",
    "                                            2: Simple Increment\\n\n",
    "                                            3: Density Coverage\\n\n",
    "                                            ''', \n",
    "                                            default_val='1')\n",
    "    manifest_genenerator.add_ait_parameters(name='combination_type', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Combination Type\\n\n",
    "                                            The default value is 0\\n\n",
    "                                            0: Implementation Not Necessary\\n\n",
    "                                            1: Execute\n",
    "                                            ''', \n",
    "                                            default_val='0')\n",
    "    manifest_genenerator.add_ait_parameters(name='combination_first', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            One Layer of Combination Coverage Target\\n\n",
    "                                            No default value\\n\n",
    "                                            (Valid and mandatory only for combination type execution)\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='combination_second', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            The Other Layer of Combination Coverage Target\\n\n",
    "                                            No default value\\n\n",
    "                                            (Valid and mandatory only for combination type execution)\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='target_scope_name', \n",
    "                                            type_='list[str]', \n",
    "                                            description='''\n",
    "                                            Names of tensorflow scopes in which neural networks are defined\\n\n",
    "                                            No default value (Mandatory)\\n\n",
    "                                            List items are separate by \",\".\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='edit_num', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Number of data manipulated at a time\\n\n",
    "                                            The default value is 100\n",
    "                                            ''', \n",
    "                                            default_val='100')\n",
    "    manifest_genenerator.add_ait_parameters(name='target_rate', \n",
    "                                            type_='float', \n",
    "                                            description='''\n",
    "                                            Target Coverage Rate (Execution terminates when the coverage reaches Target Coverage Rate)\\n\n",
    "                                            The default value is 1.0\n",
    "                                            ''', \n",
    "                                            default_val='1.0')\n",
    "    manifest_genenerator.add_ait_parameters(name='increase_rate', \n",
    "                                            type_='float', \n",
    "                                            description='''\n",
    "                                            Expected Coverage Growth Rate (if the coverage growth rage compared to 5 times before is less than Expected Coverage Growth Rate, unused manipulations are preferentially selected)\\n\n",
    "                                            The default value is 0.0\n",
    "                                            ''', \n",
    "                                            default_val='0')\n",
    "    manifest_genenerator.add_ait_parameters(name='dataset_x_num', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Number of Input Data Placeholders (in the given dataset)\\n\n",
    "                                            No default value (Mandatory)\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='dataset_y_num', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Number of Label Data Placeholders (in the given dataset)\\n\n",
    "                                            No default value (Mandatory)\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='dataset_k_num', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Number of Constant Data Placeholders (in the given dataset)\\n\n",
    "                                            No default value (Mandatory)\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_parameters(name='split_dataset_start', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Dataset Division Start Position (Data from Start Position to End Position of the given dataset are used for coverage testing)\\n\n",
    "                                            The default value is 0\n",
    "                                            ''', \n",
    "                                            default_val='0')\n",
    "    manifest_genenerator.add_ait_parameters(name='split_dataset_end', \n",
    "                                            type_='int', \n",
    "                                            description='''\n",
    "                                            Dataset Division End Position (Data from Start Position to End Position of the given dataset are used for coverage testing)\\n\n",
    "                                            The default value is the size of the given dataset\n",
    "                                            ''', \n",
    "                                            default_val='100')\n",
    "    manifest_genenerator.add_ait_parameters(name='implement_class_name', \n",
    "                                            type_='str', \n",
    "                                            description='''\n",
    "                                            Class Name in which function 'get_atomic_manipulations' is implemented\\n\n",
    "                                            No default value (Mandatory)\n",
    "                                            ''')\n",
    "    manifest_genenerator.add_ait_measures(name='coverage_rate_all_layer', \n",
    "                                          type_='float', \n",
    "                                          description='coverage of the model as a whole.', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_measures(name='coverage_rate_each_layer', \n",
    "                                          type_='float', \n",
    "                                          description='coverage of each layer in the model.', \n",
    "                                          structure='sequence')\n",
    "    manifest_genenerator.add_ait_measures(name='coverage_rate_combination', \n",
    "                                          type_='float', \n",
    "                                          description='coverage of select combination.', \n",
    "                                          structure='single')\n",
    "    manifest_genenerator.add_ait_resources(name='test_case_generator', \n",
    "                                           path='/usr/local/qai/resources/1/test_case_generator.csv', \n",
    "                                           type_='table', \n",
    "                                           description='generate coverage increase data.')\n",
    "    manifest_genenerator.add_ait_downloads(name='heatmap', \n",
    "                                           path='/usr/local/qai/downloads/1/heatmap.html', \n",
    "                                           description='the heat map of the coverage as an HTML file.')\n",
    "    manifest_genenerator.add_ait_downloads(name='abs_dataset', \n",
    "                                           path='/usr/local/qai/downloads/2/output.h5', \n",
    "                                           description='the created input data by the manipulation as h5 file.')\n",
    "    manifest_genenerator.add_ait_downloads(name='Log', \n",
    "                                           path='/usr/local/qai/downloads/3/ait.log', \n",
    "                                           description='AITLog')\n",
    "    manifest_path = manifest_genenerator.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:create input\n",
    "# should edit\n",
    "#########################################\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_input_generator import AITInputGenerator\n",
    "    input_generator = AITInputGenerator(manifest_path)\n",
    "    input_generator.add_ait_inventories(name='image_data',\n",
    "                                        value='MNIST_data/image/train-images-idx3-ubyte.gz')\n",
    "    input_generator.add_ait_inventories(name='label',\n",
    "                                        value='MNIST_data/label/train-labels-idx1-ubyte.gz')\n",
    "    input_generator.add_ait_inventories(name='tf_ckpt',\n",
    "                                        value='tf_ckpt')\n",
    "    input_generator.set_ait_params(name='mnist_image_px_size',\n",
    "                                   value='28')\n",
    "    input_generator.set_ait_params(name='determination_on_activation',\n",
    "                                   value='0')\n",
    "    input_generator.set_ait_params(name='threshold',\n",
    "                                   value='0.5')\n",
    "    input_generator.set_ait_params(name='lower_bound',\n",
    "                                   value='0.5')\n",
    "    input_generator.set_ait_params(name='upper_bound',\n",
    "                                   value='1')\n",
    "    input_generator.set_ait_params(name='activation_filter_no',\n",
    "                                   value='10')\n",
    "    input_generator.set_ait_params(name='heat_map_type',\n",
    "                                   value='1')\n",
    "    input_generator.set_ait_params(name='combination_type',\n",
    "                                   value='1')\n",
    "    input_generator.set_ait_params(name='combination_first',\n",
    "                                   value='4')\n",
    "    input_generator.set_ait_params(name='combination_second',\n",
    "                                   value='5')\n",
    "    input_generator.set_ait_params(name='target_scope_name',\n",
    "                                   value='conv1,conv2,conv3,conv4,conv5,conv6,fc1')\n",
    "    input_generator.set_ait_params(name='edit_num',\n",
    "                                   value='10')\n",
    "    input_generator.set_ait_params(name='target_rate',\n",
    "                                   value='1.0')\n",
    "    input_generator.set_ait_params(name='increase_rate',\n",
    "                                   value='0.0')\n",
    "    input_generator.set_ait_params(name='dataset_x_num',\n",
    "                                   value='1')\n",
    "    input_generator.set_ait_params(name='dataset_y_num',\n",
    "                                   value='1')\n",
    "    input_generator.set_ait_params(name='dataset_k_num',\n",
    "                                   value='1')\n",
    "    input_generator.set_ait_params(name='split_dataset_start',\n",
    "                                   value='0')\n",
    "    input_generator.set_ait_params(name='split_dataset_end',\n",
    "                                   value='100')\n",
    "    input_generator.set_ait_params(name='implement_class_name',\n",
    "                                   value='Tutorial')\n",
    "    input_generator.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:initialize\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "ait_manifest = AITManifest()\n",
    "ait_input = AITInput(ait_manifest)\n",
    "ait_output = AITOutput(ait_manifest)\n",
    "\n",
    "if is_ait_launch:\n",
    "    # launch from AIT\n",
    "    current_dir = path.dirname(path.abspath(__file__))\n",
    "    path_helper = AITPathHelper(argv=sys.argv, ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "else:\n",
    "    # launch from jupyter notebook\n",
    "    # ait.input.json make in input_dir\n",
    "    input_dir = '/usr/local/qai/mnt/ip/job_args/1/1'\n",
    "    current_dir = %pwd\n",
    "    path_helper = AITPathHelper(argv=['', input_dir], ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "\n",
    "ait_input.read_json(path_helper.get_input_file_path())\n",
    "ait_manifest.read_json(path_helper.get_manifest_file_path())\n",
    "\n",
    "### do not edit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_list(list_dict):\n",
    "    return [float(list(l.values())[0]) for l in list_dict]\n",
    "\n",
    "def add_config_json(name, ait_input, config_json) -> None:\n",
    "    value = ait_input.get_method_param_value(name)\n",
    "    config_json[name] = value\n",
    "\n",
    "def create_config_json(ait_input):\n",
    "    config_json = {}\n",
    "\n",
    "    add_config_json('determination_on_activation', ait_input, config_json)\n",
    "    add_config_json('threshold', ait_input, config_json)\n",
    "    add_config_json('lower_bound', ait_input, config_json)\n",
    "    add_config_json('upper_bound', ait_input, config_json)\n",
    "    add_config_json('heat_map_type', ait_input, config_json)\n",
    "    add_config_json('activation_filter_no', ait_input, config_json)\n",
    "    add_config_json('combination_type', ait_input, config_json)\n",
    "    add_config_json('combination_first', ait_input, config_json)\n",
    "    add_config_json('combination_second', ait_input, config_json)\n",
    "    add_config_json('target_scope_name', ait_input, config_json)\n",
    "    add_config_json('edit_num', ait_input, config_json)\n",
    "    add_config_json('target_rate', ait_input, config_json)\n",
    "    add_config_json('increase_rate', ait_input, config_json)\n",
    "    config_json['output_file_name'] = 'examples/output.h5'\n",
    "    config_json['network_structure_path'] = 'examples/tf_ckpt/model.ckpt_name.json'\n",
    "    add_config_json('dataset_x_num', ait_input, config_json)\n",
    "    add_config_json('dataset_y_num', ait_input, config_json)\n",
    "    add_config_json('dataset_k_num', ait_input, config_json)\n",
    "    add_config_json('split_dataset_start', ait_input, config_json)\n",
    "    add_config_json('split_dataset_end', ait_input, config_json)\n",
    "    add_config_json('implement_class_name', ait_input, config_json)\n",
    "    \n",
    "    return config_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'coverage_rate_all_layer')\n",
    "def calc_coverage_rate_all_layer(coverage_rate):\n",
    "    return np.mean(get_value_list(coverage_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'coverage_rate_each_layer', is_many=True)\n",
    "def calc_coverage_rate_each_layer(coverage_rate):\n",
    "    return get_value_list(coverage_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'coverage_rate_combination')\n",
    "def calc_coverage_rate_combination(combination_cov):\n",
    "    return list(combination_cov.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'heatmap')\n",
    "def save_heatmap(result_heatmap_output, file_path: str=None) -> None:\n",
    "    makedirs(str(Path(file_path).parent), exist_ok=True)\n",
    "    shutil.copyfile(result_heatmap_output, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@resources(ait_output, path_helper, 'test_case_generator')\n",
    "def save_test_case_generator(result_test_case_generator, file_path: str=None) -> None:\n",
    "    makedirs(str(Path(file_path).parent), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(result_test_case_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'Log')\n",
    "def move_log(file_path: str=None) -> None:\n",
    "    makedirs(str(Path(file_path).parent), exist_ok=True)\n",
    "\n",
    "    shutil.move(get_log_path(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:functions\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'abs_dataset')\n",
    "def save_abs_dataset(result_abs_dataset_pass, file_path: str=None) -> None:\n",
    "    makedirs(str(Path(file_path).parent), exist_ok=True)\n",
    "    shutil.copyfile(result_abs_dataset_pass, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:main\n",
    "# should edit\n",
    "#########################################\n",
    "\n",
    "@log(logger)\n",
    "@ait_main(ait_output, path_helper)\n",
    "def main() -> None:\n",
    "\n",
    "    image_px_size = ait_input.get_method_param_value('mnist_image_px_size')\n",
    "    \n",
    "    mnist = MNIST()\n",
    "    X_test = mnist.load_image(ait_input.get_inventory_path('image_data'), image_px_size)\n",
    "    y_test = mnist.load_label(ait_input.get_inventory_path('label'))\n",
    "    \n",
    "    # reshape for dnn coverage\n",
    "    X_test = X_test.reshape([X_test.shape[0], image_px_size*image_px_size])\n",
    "    \n",
    "    # onehot for dnn coverage\n",
    "    class_num = len(np.unique(y_test))\n",
    "    y_test = np.identity(class_num)[y_test]\n",
    "    \n",
    "    data_set = [\n",
    "        # x_input\n",
    "        X_test,\n",
    "        # y_input\n",
    "        y_test,\n",
    "        # k_input\n",
    "        1.0\n",
    "    ]\n",
    "    \n",
    "    session = tf.Session()\n",
    "\n",
    "    tf_ckpt_path = ait_input.get_inventory_path('tf_ckpt')\n",
    "    model_meta_path = glob(f'{tf_ckpt_path}/*.meta')[0]\n",
    "    model_path = '{}/{}'.format(tf_ckpt_path, str(Path(model_meta_path).stem))\n",
    "    \n",
    "    saver = tf.train.import_meta_graph(model_meta_path)\n",
    "    saver.restore(session, str(model_path))\n",
    "\n",
    "    conf_json = create_config_json(ait_input)\n",
    "    \n",
    "    # run coverage verification\n",
    "    result_coverage_rate, result_heatmap_output, result_combination_cov_output, result_test_case_generator, result_abs_dataset_pass = \\\n",
    "        main_test(session, data_set, conf_json)\n",
    "    \n",
    "    calc_coverage_rate_all_layer(result_coverage_rate)\n",
    "    calc_coverage_rate_each_layer(result_coverage_rate)\n",
    "    calc_coverage_rate_combination(result_combination_cov_output)\n",
    "    \n",
    "    save_test_case_generator(result_test_case_generator)\n",
    "    \n",
    "    save_heatmap(result_heatmap_output)\n",
    "    save_abs_dataset(result_abs_dataset_pass)\n",
    "    move_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage rate all layer :  0.6704282407407407\n",
      "Coverage rate one layer conv1:  1.0\n",
      "Coverage rate one layer conv2:  0.49107142857142855\n",
      "Coverage rate one layer conv3:  0.4252232142857143\n",
      "Coverage rate one layer conv4:  0.5462372448979592\n",
      "Coverage rate one layer conv5:  0.6693239795918368\n",
      "Coverage rate one layer conv6:  0.7834821428571429\n",
      "Coverage rate one layer fc1:  0.75\n",
      "Multiple layers combination coverage rate 4 and 5: 0.40959658735943355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage rate sum layer : 0.672075\n",
      "Coverage rate sum layer : 0.672298\n",
      "Coverage rate sum layer : 0.672431\n",
      "Coverage rate sum layer : 0.672565\n",
      "Coverage rate sum layer : 0.672632\n",
      "Coverage rate sum layer : 0.672721\n",
      "Coverage rate sum layer : 0.67301\n",
      "Coverage rate sum layer : 0.673032\n",
      "Coverage rate sum layer : 0.673277\n",
      "Coverage rate sum layer : 0.6733\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# area:entory point\n",
    "# do not edit\n",
    "#########################################\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:license attribute set\n",
    "# should edit\n",
    "#########################################\n",
    "ait_owner='AIST'\n",
    "ait_creation_year='2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# area:prepare deproy\n",
    "# do not edit\n",
    "#########################################\n",
    "\n",
    "if not is_ait_launch:\n",
    "    from ait_sdk.deploy import prepare_deploy\n",
    "    from ait_sdk.license.license_generator import LicenseGenerator\n",
    "    \n",
    "    current_dir = %pwd\n",
    "    prepare_deploy(ait_manifest, ait_sdk_name, current_dir, requirements_path, is_remote_deploy=True)\n",
    "    \n",
    "    # output License.txt\n",
    "    license_generator = LicenseGenerator()\n",
    "    license_generator.write('../top_dir/LICENSE.txt', ait_creation_year, ait_owner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
