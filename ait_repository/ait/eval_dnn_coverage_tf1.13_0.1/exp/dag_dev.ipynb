{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作成したプログラムのテスト用jupyter notebookです "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.2.4)\n",
      "Collecting bleach==1.5.0\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.10.0)\n",
      "Collecting decorator==4.1.2\n",
      "  Downloading decorator-4.1.2-py2.py3-none-any.whl (9.1 kB)\n",
      "Collecting h5py==2.9.0\n",
      "  Downloading h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999\n",
      "  Downloading html5lib-0.9999999.tar.gz (889 kB)\n",
      "\u001b[K     |████████████████████████████████| 889 kB 9.3 MB/s eta 0:00:01     |█▌                              | 40 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Keras==2.0.8\n",
      "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[K     |████████████████████████████████| 276 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Markdown==2.6.9\n",
      "  Downloading Markdown-2.6.9.tar.gz (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==2.0.2\n",
      "  Downloading matplotlib-2.0.2-cp36-cp36m-manylinux1_x86_64.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx==1.11\n",
      "  Downloading networkx-1.11-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 11.2 MB/s eta 0:00:01     |█████▌                          | 225 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.13.3\n",
      "  Downloading numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl (17.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.0 MB 10.9 MB/s eta 0:00:01     |████████████████████▋           | 10.9 MB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting olefile==0.44\n",
      "  Downloading olefile-0.44.zip (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.20.3\n",
      "  Downloading pandas-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 9.5 MB/s eta 0:00:011   |▉                               | 614 kB 7.8 MB/s eta 0:00:04     |███████████▌                    | 8.8 MB 7.8 MB/s eta 0:00:03     |█████████████████████▏          | 16.2 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow==4.3.0\n",
      "  Downloading Pillow-4.3.0-cp36-cp36m-manylinux1_x86_64.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 9.4 MB/s eta 0:00:01     |████████████████████████▉       | 4.5 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf==3.6.1\n",
      "  Downloading protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 9.5 MB/s eta 0:00:01     |███████████████████████▏        | 829 kB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing==2.2.0\n",
      "  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil==2.6.1\n",
      "  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz==2017.2\n",
      "  Downloading pytz-2017.2-py2.py3-none-any.whl (484 kB)\n",
      "\u001b[K     |████████████████████████████████| 484 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets==0.5.2\n",
      "  Downloading PyWavelets-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 8.0 MB/s eta 0:00:01     |███▋                            | 645 kB 8.0 MB/s eta 0:00:01     |████████████████▋               | 3.0 MB 8.0 MB/s eta 0:00:01     |█████████████████████████▎      | 4.5 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML==3.12\n",
      "  Downloading PyYAML-3.12.tar.gz (253 kB)\n",
      "\u001b[K     |████████████████████████████████| 253 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image==0.14.2\n",
      "  Downloading scikit_image-0.14.2-cp36-cp36m-manylinux1_x86_64.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 7.5 MB/s eta 0:00:011     |██████████████████████▏         | 17.6 MB 14.6 MB/s eta 0:00:01     |███████████████████████████████▋| 25.0 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==0.19.0\n",
      "  Downloading scikit_learn-0.19.0-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==0.19.1\n",
      "  Downloading scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 48.2 MB 9.6 MB/s eta 0:00:011   |█████▉                          | 8.7 MB 8.3 MB/s eta 0:00:05     |████████████▍                   | 18.7 MB 9.9 MB/s eta 0:00:03     |██████████████████▋             | 28.0 MB 10.7 MB/s eta 0:00:02     |████████████████████            | 30.3 MB 10.7 MB/s eta 0:00:02     |█████████████████████████▎      | 38.0 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.11.0\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 92.5 MB 11.8 MB/s eta 0:00:01    |██▊                             | 7.8 MB 10.7 MB/s eta 0:00:08     |████▋                           | 13.2 MB 9.7 MB/s eta 0:00:09     |█████▏                          | 15.0 MB 9.7 MB/s eta 0:00:09     |███████████▎                    | 32.5 MB 11.3 MB/s eta 0:00:06     |███████████████▎                | 44.2 MB 10.1 MB/s eta 0:00:05     |████████████████▎               | 47.1 MB 13.6 MB/s eta 0:00:04     |█████████████████▎              | 50.1 MB 13.6 MB/s eta 0:00:04     |██████████████████████▍         | 64.8 MB 11.1 MB/s eta 0:00:03     |███████████████████████▍        | 67.7 MB 10.7 MB/s eta 0:00:03     |███████████████████████████████▎| 90.6 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-tensorboard==0.1.6\n",
      "  Downloading tensorflow_tensorboard-0.1.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Werkzeug==0.12.2\n",
      "  Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf==3.6.1->-r requirements.txt (line 14)) (50.3.2)\n",
      "Collecting dask[array]>=1.0.0\n",
      "  Downloading dask-2.30.0-py3-none-any.whl (848 kB)\n",
      "\u001b[K     |████████████████████████████████| 848 kB 4.7 MB/s eta 0:00:01     |██████████████████████████▎     | 696 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle>=0.2.1\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (0.30.0)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[K     |████████████████████████████████| 367 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (1.1.2)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (0.3.3)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 7.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (0.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (1.30.0)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 10.1 MB/s eta 0:00:01     |████████████████████████████▏   | 2.8 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting toolz>=0.8.2; extra == \"array\"\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 5.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Building wheels for collected packages: html5lib, Markdown, olefile, PyYAML\n",
      "  Building wheel for html5lib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for html5lib: filename=html5lib-0.9999999-py3-none-any.whl size=111295 sha256=dd55060eae1ba8647223982ca2db52255c555ba8352d28faf74476a654f8fa45\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/1c/cb/a87fd097ff74648ecc468a703001f6c7c86d8a71d459e65c98\n",
      "  Building wheel for Markdown (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Markdown: filename=Markdown-2.6.9-py3-none-any.whl size=163233 sha256=1dafc814b809b8150939891cd4590ffad824b5194609cc8617eb1b2a2cb5e9f1\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/55/17/f129d80a5d263161d52b3c282fd818317dc95986535b7ee24a\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.44-py3-none-any.whl size=52631 sha256=945dc54c54022022f0cb7d871133cefd724ad3d0961fc3b860985335a20b6141\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/77/c6/3d974ba3cb5825fb376485fc5abd5c7a427b85b187a611fecc\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-3.12-cp36-cp36m-linux_x86_64.whl size=43458 sha256=2e72be4fb455f19927f7d7a26fa142f8a948131971657ad92fc605234b634132\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/06/c3/a48dc77c7d8f60b64eaf0fffd5ee5ab8abce5d13893b73109b\n",
      "Successfully built html5lib Markdown olefile PyYAML\n",
      "Installing collected packages: six, html5lib, bleach, decorator, numpy, h5py, PyYAML, scipy, Keras, Markdown, pytz, pyparsing, python-dateutil, matplotlib, networkx, olefile, pandas, Pillow, protobuf, PyWavelets, toolz, dask, cloudpickle, scikit-image, scikit-learn, mock, tensorflow-estimator, keras-applications, Werkzeug, tensorboard, astor, tensorflow, tensorflow-tensorboard\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 3.2.1\n",
      "    Uninstalling bleach-3.2.1:\n",
      "      Successfully uninstalled bleach-3.2.1\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 4.4.2\n",
      "    Uninstalling decorator-4.4.2:\n",
      "      Successfully uninstalled decorator-4.4.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: Markdown\n",
      "    Found existing installation: Markdown 3.2.2\n",
      "    Uninstalling Markdown-3.2.2:\n",
      "      Successfully uninstalled Markdown-3.2.2\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.2\n",
      "    Uninstalling matplotlib-3.3.2:\n",
      "      Successfully uninstalled matplotlib-3.3.2\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.0.0\n",
      "    Uninstalling Pillow-8.0.0:\n",
      "      Successfully uninstalled Pillow-8.0.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.12.2\n",
      "    Uninstalling protobuf-3.12.2:\n",
      "      Successfully uninstalled protobuf-3.12.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.1\n",
      "    Uninstalling Werkzeug-1.0.1:\n",
      "      Successfully uninstalled Werkzeug-1.0.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed Keras-2.0.8 Markdown-2.6.9 Pillow-4.3.0 PyWavelets-0.5.2 PyYAML-3.12 Werkzeug-0.12.2 astor-0.8.1 bleach-1.5.0 cloudpickle-1.6.0 dask-2.30.0 decorator-4.1.2 h5py-2.9.0 html5lib-0.9999999 keras-applications-1.0.8 matplotlib-2.0.2 mock-4.0.2 networkx-1.11 numpy-1.13.3 olefile-0.44 pandas-0.20.3 protobuf-3.6.1 pyparsing-2.2.0 python-dateutil-2.6.1 pytz-2017.2 scikit-image-0.14.2 scikit-learn-0.19.0 scipy-0.19.1 six-1.11.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 tensorflow-tensorboard-0.1.6 toolz-0.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "work_dir = Path.cwd() / 'workdir'\n",
    "if not work_dir.exists():\n",
    "    work_dir.mkdir()\n",
    "\n",
    "# prepare args\n",
    "args_dir = work_dir / 'args'\n",
    "if not args_dir.exists():\n",
    "    args_dir.mkdir()\n",
    "\n",
    "# prepare inventory\n",
    "inventory_dir = work_dir / 'inventory'\n",
    "if not inventory_dir.exists():\n",
    "    inventory_dir.mkdir()\n",
    "\n",
    "# prepare result\n",
    "result_dir = work_dir / 'result'\n",
    "if not result_dir.exists():\n",
    "    result_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_file_path = inventory_dir / 'dataset' / 'MNIST_data'\n",
    "if not dummy_file_path.parent.exists():\n",
    "    dummy_file_path.parent.mkdir()\n",
    "\n",
    "model_file_path = inventory_dir / 'tf_ckpt' / 'model.ckpt'\n",
    "\n",
    "# with open(str(dummy_file_path), mode='w') as f:\n",
    "#     f.write('dummy_inventory,hello_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "args_file = args_dir / 'args.json'\n",
    "args_file_str = str(args_file)\n",
    "\n",
    "args_json = {}\n",
    "\n",
    "mounts = []\n",
    "mounts.append({\"name\": \"args_dir\", \"dst_path\": str(args_dir)})\n",
    "mounts.append({\"name\": \"result_dir\", \"dst_path\": str(result_dir)})\n",
    "mounts.append({\"name\": \"inv_data_set\", \"dst_path\": str(dummy_file_path)})\n",
    "args_json['mounts'] = mounts\n",
    "\n",
    "method_params = []\n",
    "method_params.append({'name': 'aaa', 'value': 'bbb'})\n",
    "args_json['method_params'] = {\n",
    "\t\"determination_on_activation\": 0,\n",
    "\t\"threshold\":0.5,\n",
    "\t\"lower_bound\": 0.5,\n",
    "\t\"upper_bound\": 1,\n",
    "\t\"heat_map_type\": 1,\n",
    "\t\"activation_filter_no\": 10,\n",
    "\t\"combination_type\": 1,\n",
    "\t\"combination_first\": 4,\n",
    "\t\"combination_second\": 5,\n",
    "\t\"target_scope_name\": [\n",
    "\t\t\"conv1\",\n",
    "\t\t\"conv2\",\n",
    "\t\t\"conv3\",\n",
    "\t\t\"conv4\",\n",
    "\t\t\"conv5\",\n",
    "\t\t\"conv6\",\n",
    "\t\t\"fc1\"\n",
    "\t],\n",
    "\t\"edit_num\": 10,\n",
    "\t\"target_rate\": 1.0,\n",
    "\t\"increase_rate\": 0.0,\n",
    "\t\"output_file_name\": \"examples/output.h5\",\n",
    "\t\"network_structure_path\": \"examples/tf_ckpt/model.ckpt_name.json\",\n",
    "\t\"dataset_x_num\": 1,\n",
    "\t\"dataset_y_num\": 1,\n",
    "\t\"dataset_k_num\": 1,\n",
    "\t\"split_dataset_start\": 0,\n",
    "\t\"split_dataset_end\": 100,\n",
    "\t\"implement_class_name\": \"Tutorial\"\n",
    "}\n",
    "\n",
    "cond_params = []\n",
    "cond_params.append({'name': 'ccc', 'value': 'ddd'})\n",
    "args_json['condition_params'] = cond_params\n",
    "\n",
    "inventories=[{'name': 'data_set', 'value': str(dummy_file_path)},{'name': 'model', 'value': str(model_file_path)}]\n",
    "args_json['inventories'] = inventories\n",
    "\n",
    "with open(str(args_file), 'w') as f:\n",
    "    json.dump(args_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage test-runner\n",
      "args[0]:entrypoint.py\n",
      "args[1]:/workdir/root/exp/workdir/args/args.json\n",
      "args_file:/workdir/root/exp/workdir/args/args.json\n",
      "True - args_file:/workdir/root/exp/workdir/args/args.json\n",
      "inventory_dir:/workdir/root/exp/workdir/args\n",
      "result_dir:/workdir/root/exp/workdir/result\n",
      "False - src_dir:/workdir/root/exp/dummy_result_data\n",
      "True - result_dir:/workdir/root/exp/workdir/result\n",
      "WARNING:tensorflow:From /workdir/root/exp/entrypoint.py:45: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /workdir/root/exp/workdir/inventory/dataset/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /workdir/root/exp/workdir/inventory/dataset/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /workdir/root/exp/workdir/inventory/dataset/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /workdir/root/exp/workdir/inventory/dataset/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /workdir/root/exp/workdir/inventory/tf_ckpt/model.ckpt\n",
      "Coverage rate all layer :  0.4702190170940171\n",
      "Coverage rate one layer conv1:  1.0\n",
      "Coverage rate one layer conv2:  0.25741390306122447\n",
      "Coverage rate one layer conv3:  0.16374362244897958\n",
      "Coverage rate one layer conv4:  0.17506377551020408\n",
      "Coverage rate one layer conv5:  0.32461734693877553\n",
      "Coverage rate one layer conv6:  0.4764030612244898\n",
      "Coverage rate one layer fc1:  0.69921875\n",
      "Multiple layers combination coverage rate 4 and 5: 0.07888650318226781\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage rate sum layer : 0.471644\n",
      "Coverage rate sum layer : 0.471911\n",
      "Coverage rate sum layer : 0.472089\n",
      "Coverage rate sum layer : 0.472244\n",
      "Coverage rate sum layer : 0.472378\n",
      "Coverage rate sum layer : 0.472512\n",
      "Coverage rate sum layer : 0.472734\n",
      "Coverage rate sum layer : 0.472845\n",
      "Coverage rate sum layer : 0.472979\n",
      "Coverage rate sum layer : 0.473157\n",
      "result_coverage_rate [{'conv1': '1.0'}, {'conv2': '0.25741390306122447'}, {'conv3': '0.16374362244897958'}, {'conv4': '0.17506377551020408'}, {'conv5': '0.32461734693877553'}, {'conv6': '0.4764030612244898'}, {'fc1': '0.69921875'}]\n",
      "result_heatmap_output /workdir/root/exp/deep_saucer/neuron_coverage/tensorflow_native/lib/heatMap/coverage_20201022055132449946.html\n",
      "result_combination_cov_output {'4 and 5': '0.07888650318226781'}\n",
      "result_test_case_generator [0.4702190170940171, 0.47164351851851855, 0.47191061253561256, 0.4720886752136752, 0.47224448005698005, 0.47237802706552706, 0.47251157407407407, 0.4727341524216524, 0.4728454415954416, 0.4729789886039886, 0.47315705128205127]\n",
      "result_abs_dataset_pass /workdir/root/exp/deep_saucer/neuron_coverage/tensorflow_native/lib/examples/output.h5\n"
     ]
    }
   ],
   "source": [
    "run entrypoint $args_file_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('path_to_my_model')\n",
    "#del model\n",
    "# Recreate the exact same model purely from the file:\n",
    "#model = keras.models.load_model('path_to_my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "glob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
