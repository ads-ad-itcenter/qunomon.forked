{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作成したプログラムのテスト用jupyter notebookです "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/2e/df11ea7e23e7e761d484ed3740285a34e38548cf2bad2bed3dd5768ec8b9/pip-20.1-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-20.1\n",
      "Collecting bleach==1.5.0\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.10.0)\n",
      "Collecting decorator==4.1.2\n",
      "  Downloading decorator-4.1.2-py2.py3-none-any.whl (9.1 kB)\n",
      "Collecting h5py==2.9.0\n",
      "  Downloading h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999\n",
      "  Downloading html5lib-0.9999999.tar.gz (889 kB)\n",
      "\u001b[K     |████████████████████████████████| 889 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Keras==2.0.8\n",
      "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[K     |████████████████████████████████| 276 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Markdown==2.6.9\n",
      "  Downloading Markdown-2.6.9.tar.gz (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==2.0.2\n",
      "  Downloading matplotlib-2.0.2-cp36-cp36m-manylinux1_x86_64.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx==1.11\n",
      "  Downloading networkx-1.11-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.13.3\n",
      "  Downloading numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl (17.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.0 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting olefile==0.44\n",
      "  Downloading olefile-0.44.zip (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.20.3\n",
      "  Downloading pandas-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow==4.3.0\n",
      "  Downloading Pillow-4.3.0-cp36-cp36m-manylinux1_x86_64.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf==3.6.1\n",
      "  Downloading protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing==2.2.0\n",
      "  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil==2.6.1\n",
      "  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz==2017.2\n",
      "  Downloading pytz-2017.2-py2.py3-none-any.whl (484 kB)\n",
      "\u001b[K     |████████████████████████████████| 484 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets==0.5.2\n",
      "  Downloading PyWavelets-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML==3.12\n",
      "  Downloading PyYAML-3.12.tar.gz (253 kB)\n",
      "\u001b[K     |████████████████████████████████| 253 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image==0.14.2\n",
      "  Downloading scikit_image-0.14.2-cp36-cp36m-manylinux1_x86_64.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 5.3 MB/s eta 0:00:011     |█████████████████████████████▊  | 23.5 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==0.19.0\n",
      "  Downloading scikit_learn-0.19.0-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==0.19.1\n",
      "  Downloading scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 48.2 MB 8.5 MB/s eta 0:00:01     |█████████████████████████████▊  | 44.8 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.11.0\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 92.5 MB 444 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-tensorboard==0.1.6\n",
      "  Downloading tensorflow_tensorboard-0.1.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Werkzeug==0.12.2\n",
      "  Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf==3.6.1->-r requirements.txt (line 14)) (44.0.0)\n",
      "Collecting cloudpickle>=0.2.1\n",
      "  Downloading cloudpickle-1.4.1-py3-none-any.whl (26 kB)\n",
      "Collecting dask[array]>=1.0.0\n",
      "  Downloading dask-2.15.0-py3-none-any.whl (799 kB)\n",
      "\u001b[K     |████████████████████████████████| 799 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[K     |████████████████████████████████| 367 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (1.0.8)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (1.26.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (0.9.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 24)) (0.30.0)\n",
      "Collecting toolz>=0.8.2; extra == \"array\"\n",
      "  Downloading toolz-0.10.0.tar.gz (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Building wheels for collected packages: html5lib, Markdown, olefile, PyYAML, toolz\n",
      "  Building wheel for html5lib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for html5lib: filename=html5lib-0.9999999-py3-none-any.whl size=111295 sha256=2b3672d547e080879fd64644eb8736962a919f52c440fe1ca90493a8d529df5c\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/1c/cb/a87fd097ff74648ecc468a703001f6c7c86d8a71d459e65c98\n",
      "  Building wheel for Markdown (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Markdown: filename=Markdown-2.6.9-py3-none-any.whl size=163238 sha256=abcc536219f4f5f624220b7fa266466561b6c788b33c5c0f709c0ddad4e4cca4\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/55/17/f129d80a5d263161d52b3c282fd818317dc95986535b7ee24a\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.44-py3-none-any.whl size=52631 sha256=db0ae037dd21705894e4b018d98d989bb6f602b4c8243739da8751a4f8f2a374\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/77/c6/3d974ba3cb5825fb376485fc5abd5c7a427b85b187a611fecc\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-3.12-cp36-cp36m-linux_x86_64.whl size=43458 sha256=44250c9a50937f6ca382a68e18f076e7aeb8086d67716e500647f4216eb758ef\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/06/c3/a48dc77c7d8f60b64eaf0fffd5ee5ab8abce5d13893b73109b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for toolz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for toolz: filename=toolz-0.10.0-py3-none-any.whl size=57158 sha256=abe16e98ece124f868e4eb66c37e07db65637471cfbe7c1f3eaee1096361eabe\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/da/2e/27e381e9cfc922d078a0a750c7ec72e76df66100e81722516d\n",
      "Successfully built html5lib Markdown olefile PyYAML toolz\n",
      "Installing collected packages: six, html5lib, bleach, decorator, numpy, h5py, PyYAML, scipy, Keras, Markdown, pytz, python-dateutil, pyparsing, matplotlib, networkx, olefile, pandas, Pillow, protobuf, PyWavelets, cloudpickle, toolz, dask, scikit-image, scikit-learn, mock, tensorflow-estimator, Werkzeug, tensorboard, tensorflow, tensorflow-tensorboard\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.13.0\n",
      "    Uninstalling six-1.13.0:\n",
      "      Successfully uninstalled six-1.13.0\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 3.1.0\n",
      "    Uninstalling bleach-3.1.0:\n",
      "      Successfully uninstalled bleach-3.1.0\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 4.4.1\n",
      "    Uninstalling decorator-4.4.1:\n",
      "      Successfully uninstalled decorator-4.4.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Uninstalling numpy-1.18.1:\n",
      "      Successfully uninstalled numpy-1.18.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: Markdown\n",
      "    Found existing installation: Markdown 3.1.1\n",
      "    Uninstalling Markdown-3.1.1:\n",
      "      Successfully uninstalled Markdown-3.1.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.6\n",
      "    Uninstalling pyparsing-2.4.6:\n",
      "      Successfully uninstalled pyparsing-2.4.6\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.1.2\n",
      "    Uninstalling matplotlib-3.1.2:\n",
      "      Successfully uninstalled matplotlib-3.1.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.11.2\n",
      "    Uninstalling protobuf-3.11.2:\n",
      "      Successfully uninstalled protobuf-3.11.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 0.16.0\n",
      "    Uninstalling Werkzeug-0.16.0:\n",
      "      Successfully uninstalled Werkzeug-0.16.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.1.0\n",
      "    Uninstalling tensorboard-2.1.0:\n",
      "      Successfully uninstalled tensorboard-2.1.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed Keras-2.0.8 Markdown-2.6.9 Pillow-4.3.0 PyWavelets-0.5.2 PyYAML-3.12 Werkzeug-0.12.2 bleach-1.5.0 cloudpickle-1.4.1 dask-2.15.0 decorator-4.1.2 h5py-2.9.0 html5lib-0.9999999 matplotlib-2.0.2 mock-4.0.2 networkx-1.11 numpy-1.13.3 olefile-0.44 pandas-0.20.3 protobuf-3.6.1 pyparsing-2.2.0 python-dateutil-2.6.1 pytz-2017.2 scikit-image-0.14.2 scikit-learn-0.19.0 scipy-0.19.1 six-1.11.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 tensorflow-tensorboard-0.1.6 toolz-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "work_dir = Path.cwd() / 'workdir'\n",
    "if not work_dir.exists():\n",
    "    work_dir.mkdir()\n",
    "\n",
    "# prepare args\n",
    "args_dir = work_dir / 'args'\n",
    "if not args_dir.exists():\n",
    "    args_dir.mkdir()\n",
    "\n",
    "# prepare inventory\n",
    "inventory_dir = work_dir / 'inventory'\n",
    "if not inventory_dir.exists():\n",
    "    inventory_dir.mkdir()\n",
    "\n",
    "# prepare result\n",
    "result_dir = work_dir / 'result'\n",
    "if not result_dir.exists():\n",
    "    result_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_file_path = inventory_dir / 'dataset' / 'MNIST_data'\n",
    "if not dummy_file_path.parent.exists():\n",
    "    dummy_file_path.parent.mkdir()\n",
    "\n",
    "model_file_path = inventory_dir / 'tf_ckpt' / 'model.ckpt'\n",
    "\n",
    "# with open(str(dummy_file_path), mode='w') as f:\n",
    "#     f.write('dummy_inventory,hello_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "args_file = args_dir / 'args.json'\n",
    "args_file_str = str(args_file)\n",
    "\n",
    "args_json = {}\n",
    "\n",
    "mounts = []\n",
    "mounts.append({\"name\": \"args_dir\", \"dst_path\": str(args_dir)})\n",
    "mounts.append({\"name\": \"result_dir\", \"dst_path\": str(result_dir)})\n",
    "mounts.append({\"name\": \"inv_data_set\", \"dst_path\": str(dummy_file_path)})\n",
    "args_json['mounts'] = mounts\n",
    "\n",
    "method_params = []\n",
    "method_params.append({'name': 'aaa', 'value': 'bbb'})\n",
    "args_json['method_params'] = {\n",
    "\t\"determination_on_activation\": 0,\n",
    "\t\"threshold\":0.5,\n",
    "\t\"lower_bound\": 0.5,\n",
    "\t\"upper_bound\": 1,\n",
    "\t\"heat_map_type\": 1,\n",
    "\t\"activation_filter_no\": 10,\n",
    "\t\"combination_type\": 1,\n",
    "\t\"combination_first\": 4,\n",
    "\t\"combination_second\": 5,\n",
    "\t\"target_scope_name\": [\n",
    "\t\t\"conv1\",\n",
    "\t\t\"conv2\",\n",
    "\t\t\"conv3\",\n",
    "\t\t\"conv4\",\n",
    "\t\t\"conv5\",\n",
    "\t\t\"conv6\",\n",
    "\t\t\"fc1\"\n",
    "\t],\n",
    "\t\"edit_num\": 10,\n",
    "\t\"target_rate\": 1.0,\n",
    "\t\"increase_rate\": 0.0,\n",
    "\t\"output_file_name\": \"examples/output.h5\",\n",
    "\t\"network_structure_path\": \"examples/tf_ckpt/model.ckpt_name.json\",\n",
    "\t\"dataset_x_num\": 1,\n",
    "\t\"dataset_y_num\": 1,\n",
    "\t\"dataset_k_num\": 1,\n",
    "\t\"split_dataset_start\": 0,\n",
    "\t\"split_dataset_end\": 100,\n",
    "\t\"implement_class_name\": \"Tutorial\"\n",
    "}\n",
    "\n",
    "cond_params = []\n",
    "cond_params.append({'name': 'ccc', 'value': 'ddd'})\n",
    "args_json['condition_params'] = cond_params\n",
    "\n",
    "inventories=[{'name': 'data_set', 'value': str(dummy_file_path)},{'name': 'model', 'value': str(model_file_path)}]\n",
    "args_json['inventories'] = inventories\n",
    "\n",
    "with open(str(args_file), 'w') as f:\n",
    "    json.dump(args_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage test-runner\n",
      "args[0]:entrypoint.py\n",
      "args[1]:/tf/repository/workdir/args/args.json\n",
      "args_file:/tf/repository/workdir/args/args.json\n",
      "True - args_file:/tf/repository/workdir/args/args.json\n",
      "inventory_dir:/tf/repository/workdir/args\n",
      "result_dir:/tf/repository/workdir/result\n",
      "False - src_dir:/tf/repository/dummy_result_data\n",
      "True - result_dir:/tf/repository/workdir/result\n",
      "WARNING:tensorflow:From /tf/repository/entrypoint.py:45: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tf/repository/workdir/inventory/dataset/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tf/repository/workdir/inventory/dataset/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tf/repository/workdir/inventory/dataset/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tf/repository/workdir/inventory/dataset/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tf/repository/workdir/inventory/tf_ckpt/model.ckpt\n",
      "Coverage rate all layer :  0.4702190170940171\n",
      "Coverage rate one layer conv1:  1.0\n",
      "Coverage rate one layer conv2:  0.25741390306122447\n",
      "Coverage rate one layer conv3:  0.16374362244897958\n",
      "Coverage rate one layer conv4:  0.17506377551020408\n",
      "Coverage rate one layer conv5:  0.32461734693877553\n",
      "Coverage rate one layer conv6:  0.4764030612244898\n",
      "Coverage rate one layer fc1:  0.69921875\n",
      "Multiple layers combination coverage rate 4 and 5: 0.07888650318226781\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage rate sum layer : 0.471644\n",
      "Coverage rate sum layer : 0.471911\n",
      "Coverage rate sum layer : 0.472089\n",
      "Coverage rate sum layer : 0.472244\n",
      "Coverage rate sum layer : 0.472378\n",
      "Coverage rate sum layer : 0.472512\n",
      "Coverage rate sum layer : 0.472734\n",
      "Coverage rate sum layer : 0.472845\n",
      "Coverage rate sum layer : 0.472979\n",
      "Coverage rate sum layer : 0.473157\n",
      "result_coverage_rate [{'conv1': '1.0'}, {'conv2': '0.25741390306122447'}, {'conv3': '0.16374362244897958'}, {'conv4': '0.17506377551020408'}, {'conv5': '0.32461734693877553'}, {'conv6': '0.4764030612244898'}, {'fc1': '0.69921875'}]\n",
      "result_heatmap_output /tf/repository/deep_saucer/neuron_coverage/tensorflow_native/lib/heatMap/coverage_20200508084628363973.html\n",
      "result_combination_cov_output {'4 and 5': '0.07888650318226781'}\n",
      "result_test_case_generator [0.4702190170940171, 0.47164351851851855, 0.47191061253561256, 0.4720886752136752, 0.47224448005698005, 0.47237802706552706, 0.47251157407407407, 0.4727341524216524, 0.4728454415954416, 0.4729789886039886, 0.47315705128205127]\n",
      "result_abs_dataset_pass /tf/repository/deep_saucer/neuron_coverage/tensorflow_native/lib/examples/output.h5\n"
     ]
    }
   ],
   "source": [
    "run entrypoint $args_file_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('path_to_my_model')\n",
    "#del model\n",
    "# Recreate the exact same model purely from the file:\n",
    "#model = keras.models.load_model('path_to_my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
